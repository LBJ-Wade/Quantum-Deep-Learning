{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- coding: utf-8 --\n",
    "# This code is part of Qiskit.\n",
    "#\n",
    "# (C) Copyright IBM 2019.\n",
    "#\n",
    "# This code is licensed under the Apache License, Version 2.0. You may\n",
    "# obtain a copy of this license in the LICENSE.txt file in the root directory\n",
    "# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "#\n",
    "# Any modifications or derivative works of this code must retain this\n",
    "# copyright notice, and modified files need to carry a notice indicating\n",
    "# that they have been altered from the originals.\n",
    "#\n",
    "# Code adapted from QizGloria team, Qiskit Camp Europe 2019, updated by \n",
    "# Team Ube Pancake, Qiskit Summer Jam 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumRegister,QuantumCircuit,ClassicalRegister,execute\n",
    "from qiskit.circuit import Parameter,ControlledGate\n",
    "from qiskit import Aer\n",
    "import qiskit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "NUM_QUBITS = 4\n",
    "NUM_SHOTS = 10000\n",
    "SHIFT = np.pi/8\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "SIMULATOR = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to translate Q-Circuit parameters from pytorch back to QISKIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numbers(tensor_list):\n",
    "    num_list = []\n",
    "    for tensor in tensor_list:\n",
    "        num_list += [tensor.item()]\n",
    "    return num_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Contruct QuantumCircuit QFT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T16:09:30.598730Z",
     "start_time": "2019-10-01T16:09:30.567861Z"
    }
   },
   "outputs": [],
   "source": [
    "class QiskitCircuit():\n",
    "    \n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        # --- Circuit definition ---\n",
    "        self.circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        self.n_qubits = n_qubits\n",
    "        self.theta0 = Parameter('Theta0')\n",
    "        self.theta1 = Parameter('Theta1')\n",
    "        self.theta2 = Parameter('Theta2')\n",
    "        self.theta3 = Parameter('Theta3')\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        self.circuit.h(all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        self.circuit.ry(self.theta0, 0)\n",
    "        self.circuit.ry(self.theta1, 1)\n",
    "        self.circuit.ry(self.theta2, 2)\n",
    "        self.circuit.ry(self.theta3, 3)\n",
    "        self.circuit.barrier()\n",
    "        \n",
    "#         # Apply controlled-unitary\n",
    "# #         uc=ry(self.theta4, 4).to_gate().control(4)\n",
    "# #         self.circuit.append(uc, [0,1,2,3,4])\n",
    "#         self.circuit.ry(self.theta4, 4).to_gate().control(4)\n",
    "    \n",
    "        self.circuit.barrier()\n",
    "        self.circuit.measure_all()\n",
    "        # ---------------------------\n",
    "        \n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "        \n",
    "    def N_qubit_expectation_Z(self,counts, shots, nr_qubits):\n",
    "        expects = np.zeros(nr_qubits)\n",
    "        for key in counts.keys():\n",
    "            perc = counts[key]/shots\n",
    "            check = np.array([(float(key[i])-1/2)*2*perc for i in range(nr_qubits)])\n",
    "            expects += check   \n",
    "        return expects  \n",
    "    \n",
    "    def run(self, i):\n",
    "        params = i[0]\n",
    "#         print('params = {}'.format(len(params)))\n",
    "        backend = Aer.get_backend('qasm_simulator')\n",
    "        job_sim = execute(self.circuit,\n",
    "                          self.backend,\n",
    "                          shots=self.shots,\n",
    "                          parameter_binds = [{self.theta0 : float(params[0].item()),\n",
    "                                              self.theta1 : float(params[1].item()),\n",
    "                                              self.theta2 : float(params[2].item()),\n",
    "                                              self.theta3 : float(params[3].item())}])\n",
    "        \n",
    "        result_sim = job_sim.result()\n",
    "        counts = result_sim.get_counts(self.circuit)\n",
    "        return self.N_qubit_expectation_Z(counts,self.shots,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation [pi/4]: [ 0.7126 -0.7124  0.7174 -0.7096]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌───┐ ░ ┌────────────┐ ░  ░  ░ ┌─┐         \n",
       "   q_0: ┤ H ├─░─┤ RY(Theta0) ├─░──░──░─┤M├─────────\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░ └╥┘┌─┐      \n",
       "   q_1: ┤ H ├─░─┤ RY(Theta1) ├─░──░──░──╫─┤M├──────\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░  ║ └╥┘┌─┐   \n",
       "   q_2: ┤ H ├─░─┤ RY(Theta2) ├─░──░──░──╫──╫─┤M├───\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░  ║  ║ └╥┘┌─┐\n",
       "   q_3: ┤ H ├─░─┤ RY(Theta3) ├─░──░──░──╫──╫──╫─┤M├\n",
       "        └───┘ ░ └────────────┘ ░  ░  ░  ║  ║  ║ └╥┘\n",
       "meas_0: ════════════════════════════════╩══╬══╬══╬═\n",
       "                                           ║  ║  ║ \n",
       "meas_1: ═══════════════════════════════════╩══╬══╬═\n",
       "                                              ║  ║ \n",
       "meas_2: ══════════════════════════════════════╩══╬═\n",
       "                                                 ║ \n",
       "meas_3: ═════════════════════════════════════════╩═\n",
       "                                                   </pre>"
      ],
      "text/plain": [
       "        ┌───┐ ░ ┌────────────┐ ░  ░  ░ ┌─┐         \n",
       "   q_0: ┤ H ├─░─┤ RY(Theta0) ├─░──░──░─┤M├─────────\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░ └╥┘┌─┐      \n",
       "   q_1: ┤ H ├─░─┤ RY(Theta1) ├─░──░──░──╫─┤M├──────\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░  ║ └╥┘┌─┐   \n",
       "   q_2: ┤ H ├─░─┤ RY(Theta2) ├─░──░──░──╫──╫─┤M├───\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░  ║  ║ └╥┘┌─┐\n",
       "   q_3: ┤ H ├─░─┤ RY(Theta3) ├─░──░──░──╫──╫──╫─┤M├\n",
       "        └───┘ ░ └────────────┘ ░  ░  ░  ║  ║  ║ └╥┘\n",
       "meas_0: ════════════════════════════════╩══╬══╬══╬═\n",
       "                                           ║  ║  ║ \n",
       "meas_1: ═══════════════════════════════════╩══╬══╬═\n",
       "                                              ║  ║ \n",
       "meas_2: ══════════════════════════════════════╩══╬═\n",
       "                                                 ║ \n",
       "meas_3: ═════════════════════════════════════════╩═\n",
       "                                                   "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = QiskitCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)\n",
    "print('Expected value for rotation [pi/4]: {}'.format(circuit.run(torch.Tensor([[-np.pi/4, np.pi/4, -np.pi/4, np.pi/4]]))))\n",
    "circuit.circuit.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TorchCircuit()\n",
    "\n",
    "A pytorch layer always has two functions. One for the forward pass and one for the backward pass. The forward pass simply takes the Quantum Circuits variational parameters from the previous pytorch layer and runs the circuit on the defined hardware (defined in `QiskitCircuit.run()`) and returns the measurements from the quantum hardware.\n",
    "These measurements will be the inputs of the next pytorch layer.\n",
    "\n",
    "The backward pass returns the gradients of the quantum circuit. In this case here it is finite difference.\n",
    "\n",
    "the `forward_tensor` is saved from the forward pass. So we just have to do one evaluation of the Q-Circuit in the backpass for the finite difference.\n",
    "\n",
    "The `gradient` variable here is as well hard coded to 3 parameters. This should be updated in the future and made more general.\n",
    "\n",
    "The loop `for k in range(len(input_numbers)):` goes through all the parameters (in this case 3), and shifts them by a small $\\epsilon$. Then it runs the circuit and takes the diefferences of the ouput for the parameters $\\Theta$ and $\\Theta + \\epsilon$. This is the finite difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCircuit(Function):    \n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'QiskitCirc'):\n",
    "            ctx.QiskitCirc = QiskitCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n",
    "            \n",
    "        exp_value = ctx.QiskitCirc.run(i)\n",
    "        \n",
    "        result = torch.tensor([exp_value])\n",
    "        \n",
    "        ctx.save_for_backward(result, i)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        eps = 0.01\n",
    "        \n",
    "        forward_tensor, i = ctx.saved_tensors\n",
    "#         print('forward_tensor = {}'.format(forward_tensor))\n",
    "        input_numbers = i\n",
    "        gradients = []\n",
    "        \n",
    "        for k in range(len(input_numbers)):\n",
    "            input_eps = input_numbers\n",
    "            input_eps[k] = input_numbers[k] + eps\n",
    "            exp_value = ctx.QiskitCirc.run(torch.Tensor(input_eps))\n",
    "            gradient = (exp_value - forward_tensor[0][k].item())#/eps\n",
    "            gradients.append(gradient)\n",
    "            \n",
    "#         print('gradient = {}'.format(gradients))\n",
    "        result = torch.tensor(gradients)\n",
    "\n",
    "        return result.float() * grad_output.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 after quantum layer: tensor([[0.7008, 0.7080, 0.7036, 0.7044]], dtype=torch.float64,\n",
      "       grad_fn=<TorchCircuitBackward>)\n",
      "x.grad = tensor([[-0.0024,  0.0028, -0.0010,  0.0023]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[np.pi/4, np.pi/4, np.pi/4, np.pi/4]], requires_grad=True)\n",
    "# x = torch.tensor([[0.0, 0.0, 0.0]], requires_grad=True)\n",
    "\n",
    "qc = TorchCircuit.apply\n",
    "y1 = qc(x)\n",
    "print('y1 after quantum layer: {}'.format(y1))\n",
    "y1 = nn.Linear(4,1)(y1.float())\n",
    "y1.backward()\n",
    "print('x.grad = {}'.format(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Quantum Circuit separately\n",
    "\n",
    "This example is simply to test the QC with a pytorch optimizer\n",
    "\n",
    "We define a cost function and a target expectation value (here -1). The cost is the square distance from the target value.\n",
    "\n",
    "`x` is the initialization of the parameters. Here again, this was hard coded such that every angle starts at $\\pi / 4$.\n",
    "\n",
    "The rest is standard pytorch optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 19.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a12b869d30>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = TorchCircuit.apply\n",
    "\n",
    "def cost(x):\n",
    "    target = -1\n",
    "    expval = qc(x)[0]\n",
    "    # simple linear layer: add up 4 outputs of quantum layer\n",
    "    val = expval[0] + expval[1] + expval[2] + expval[3]\n",
    "    return torch.abs(val - target) ** 2, expval\n",
    "\n",
    "x = torch.tensor([[np.pi/4, 0, np.pi/4, 0]], requires_grad=True)\n",
    "opt = torch.optim.Adam([x], lr=0.1)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "loss_list = []\n",
    "expval_list = []\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "# for i in range(num_epoch):\n",
    "    opt.zero_grad()\n",
    "    loss, expval = cost(x)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    expval_list.append(expval)\n",
    "#     print(loss.item())\n",
    "\n",
    "plt.plot(loss_list)\n",
    "# print('final parameters: {}'.format(expval_list))\n",
    "    \n",
    "# print(circuit(phi, theta))\n",
    "# print(cost(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST\n",
    "\n",
    "In this code we can not handle batches yet.\n",
    "This should be implemented as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 100\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size_train = 1\n",
    "batch_size_test = 1\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "n_datapoints = 100\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()])\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "labels = mnist_trainset.targets #get labels\n",
    "labels = labels.numpy()\n",
    "idx1 = np.where(labels == 0) #search all zeros\n",
    "idx2 = np.where(labels == 1) # search all ones\n",
    "idx = np.concatenate((idx1[0][0:n_datapoints//2],idx2[0][0:n_datapoints//2])) # concatenate their indices\n",
    "mnist_trainset.targets = labels[idx] \n",
    "mnist_trainset.data = mnist_trainset.data[idx]\n",
    "\n",
    "print(mnist_trainset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network with Q-node\n",
    "\n",
    "This NN is  2 layers of ConvNN and a fully connected layer, with a Q-Node as a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, NUM_QUBITS)\n",
    "        self.qft = TorchCircuit.apply\n",
    "        self.fc3 = nn.Linear(NUM_QUBITS, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = np.pi*torch.tanh(x)\n",
    "        x = self.qft(x) # This is the q node\n",
    "        x = self.fc3(x.float())\n",
    "#         print(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "#         x = torch.argmax(x)\n",
    "#         x = torch.Tensor([x])\n",
    "        print(x)\n",
    "#         x = torch.cat((x, 1-x), -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "# optimizer = optim.Adam(network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "standard pytorch training loop.\n",
    "- Load data from train_loader. Which is this case a single example each step.\n",
    "- Forward pass through NN\n",
    "- Caluculate loss\n",
    "- Backprop and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4187, 0.5813]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4584, 0.5416]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4131, 0.5869]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4334, 0.5666]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4178, 0.5822]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4042, 0.5958]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4084, 0.5916]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4208, 0.5792]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4474, 0.5526]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4452, 0.5548]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4223, 0.5777]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4264, 0.5736]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4531, 0.5469]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4687, 0.5313]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4244, 0.5756]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4060, 0.5940]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4261, 0.5739]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4040, 0.5960]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4249, 0.5751]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4143, 0.5857]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4523, 0.5477]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4455, 0.5545]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4529, 0.5471]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4247, 0.5753]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4244, 0.5756]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4082, 0.5918]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4568, 0.5432]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4396, 0.5604]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4333, 0.5667]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4361, 0.5639]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4378, 0.5622]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4176, 0.5824]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4472, 0.5528]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4494, 0.5506]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4533, 0.5467]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4485, 0.5515]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4418, 0.5582]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4555, 0.5445]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4101, 0.5899]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4224, 0.5776]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4364, 0.5636]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4452, 0.5548]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4289, 0.5711]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4345, 0.5655]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4560, 0.5440]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4667, 0.5333]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4904, 0.5096]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4706, 0.5294]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4451, 0.5549]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4634, 0.5366]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4574, 0.5426]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4748, 0.5252]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4592, 0.5408]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4607, 0.5393]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4614, 0.5386]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4664, 0.5336]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4625, 0.5375]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4528, 0.5472]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4549, 0.5451]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4178, 0.5822]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4638, 0.5362]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4179, 0.5821]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4146, 0.5854]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4639, 0.5361]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4115, 0.5885]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4544, 0.5456]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4691, 0.5309]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4398, 0.5602]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4550, 0.5450]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4577, 0.5423]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4319, 0.5681]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4580, 0.5420]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4373, 0.5627]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4128, 0.5872]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4122, 0.5878]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4622, 0.5378]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4487, 0.5513]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4471, 0.5529]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4368, 0.5632]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3960, 0.6040]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4570, 0.5430]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4802, 0.5198]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4287, 0.5713]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4601, 0.5399]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4253, 0.5747]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4758, 0.5242]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4407, 0.5593]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4613, 0.5387]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4146, 0.5854]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4233, 0.5767]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4619, 0.5381]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4339, 0.5661]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4884, 0.5116]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4439, 0.5561]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3868, 0.6132]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4558, 0.5442]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4647, 0.5353]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4526, 0.5474]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4704, 0.5296]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4309, 0.5691]], grad_fn=<SoftmaxBackward>)\n",
      "Training [5%]\tLoss: -0.4962\n",
      "tensor([[0.4485, 0.5515]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4349, 0.5651]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4391, 0.5609]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3894, 0.6106]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4022, 0.5978]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4348, 0.5652]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4150, 0.5850]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3692, 0.6308]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3570, 0.6430]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4094, 0.5906]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3522, 0.6478]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4072, 0.5928]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3954, 0.6046]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4441, 0.5559]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4006, 0.5994]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3580, 0.6420]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4168, 0.5832]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4190, 0.5810]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4107, 0.5893]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4341, 0.5659]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3765, 0.6235]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4640, 0.5360]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4359, 0.5641]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3888, 0.6112]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4275, 0.5725]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4438, 0.5562]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4403, 0.5597]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4479, 0.5521]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5183, 0.4817]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4050, 0.5950]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3991, 0.6009]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4680, 0.5320]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5246, 0.4754]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3625, 0.6375]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4345, 0.5655]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4432, 0.5568]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4835, 0.5165]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4362, 0.5638]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4756, 0.5244]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5216, 0.4784]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4474, 0.5526]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3771, 0.6229]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4214, 0.5786]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4212, 0.5788]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4137, 0.5863]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5006, 0.4994]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4432, 0.5568]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3675, 0.6325]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4349, 0.5651]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4507, 0.5493]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3532, 0.6468]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4507, 0.5493]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2921, 0.7079]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4231, 0.5769]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4474, 0.5526]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3452, 0.6548]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3834, 0.6166]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4195, 0.5805]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3361, 0.6639]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4286, 0.5714]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3499, 0.6501]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3429, 0.6571]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4551, 0.5449]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4131, 0.5869]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4434, 0.5566]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5144, 0.4856]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3458, 0.6542]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4712, 0.5288]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3448, 0.6552]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2745, 0.7255]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2877, 0.7123]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2810, 0.7190]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4417, 0.5583]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4587, 0.5413]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4578, 0.5422]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3837, 0.6163]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5971, 0.4029]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4058, 0.5942]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3222, 0.6778]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2452, 0.7548]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4340, 0.5660]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4464, 0.5536]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4897, 0.5103]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3664, 0.6336]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2714, 0.7286]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4091, 0.5909]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4926, 0.5074]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3397, 0.6603]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6153, 0.3847]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4121, 0.5879]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3659, 0.6341]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5705, 0.4295]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7005, 0.2995]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5300, 0.4700]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6821, 0.3179]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3945, 0.6055]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6325, 0.3675]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3362, 0.6638]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3256, 0.6744]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4622, 0.5378]], grad_fn=<SoftmaxBackward>)\n",
      "Training [10%]\tLoss: -0.5305\n",
      "tensor([[0.3381, 0.6619]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2568, 0.7432]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2610, 0.7390]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5214, 0.4786]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4995, 0.5005]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2343, 0.7657]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4684, 0.5316]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3956, 0.6044]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4338, 0.5662]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3508, 0.6492]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5616, 0.4384]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4109, 0.5891]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3395, 0.6605]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4078, 0.5922]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4911, 0.5089]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3872, 0.6128]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5543, 0.4457]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4885, 0.5115]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3995, 0.6005]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4076, 0.5924]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4301, 0.5699]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6385, 0.3615]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5370, 0.4630]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5567, 0.4433]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2720, 0.7280]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3166, 0.6834]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3009, 0.6991]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5280, 0.4720]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4968, 0.5032]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3655, 0.6345]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5540, 0.4460]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5699, 0.4301]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4138, 0.5862]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6212, 0.3788]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5508, 0.4492]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4893, 0.5107]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4211, 0.5789]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4360, 0.5640]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3780, 0.6220]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4991, 0.5009]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4326, 0.5674]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3979, 0.6021]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5314, 0.4686]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5451, 0.4549]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5353, 0.4647]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6798, 0.3202]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3445, 0.6555]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3627, 0.6373]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5687, 0.4313]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6466, 0.3534]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5836, 0.4164]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3671, 0.6329]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5984, 0.4016]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4291, 0.5709]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5715, 0.4285]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3467, 0.6533]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6041, 0.3959]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5876, 0.4124]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5193, 0.4807]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5899, 0.4101]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2052, 0.7948]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3063, 0.6937]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3575, 0.6425]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4450, 0.5550]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6948, 0.3052]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4457, 0.5543]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3943, 0.6057]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6460, 0.3540]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3992, 0.6008]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7851, 0.2149]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2732, 0.7268]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5566, 0.4434]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6953, 0.3047]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6427, 0.3573]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5879, 0.4121]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6316, 0.3684]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4593, 0.5407]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5446, 0.4554]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6263, 0.3737]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5399, 0.4601]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3967, 0.6033]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3874, 0.6126]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5791, 0.4209]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3603, 0.6397]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3540, 0.6460]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6214, 0.3786]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3443, 0.6557]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4381, 0.5619]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5541, 0.4459]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3623, 0.6377]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5003, 0.4997]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6415, 0.3585]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6456, 0.3544]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4973, 0.5027]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5271, 0.4729]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5738, 0.4262]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6440, 0.3560]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5101, 0.4899]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5826, 0.4174]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7108, 0.2892]], grad_fn=<SoftmaxBackward>)\n",
      "Training [15%]\tLoss: -0.5714\n",
      "tensor([[0.3507, 0.6493]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3454, 0.6546]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5527, 0.4473]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3889, 0.6111]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6440, 0.3560]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4412, 0.5588]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3748, 0.6252]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6533, 0.3467]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5682, 0.4318]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1437, 0.8563]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7248, 0.2752]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5262, 0.4738]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4627, 0.5373]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5835, 0.4165]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7788, 0.2212]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4125, 0.5875]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4944, 0.5056]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6740, 0.3260]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6171, 0.3829]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4347, 0.5653]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5980, 0.4020]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3267, 0.6733]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6348, 0.3652]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5826, 0.4174]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6773, 0.3227]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5488, 0.4512]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4866, 0.5134]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1311, 0.8689]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3374, 0.6626]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5591, 0.4409]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5300, 0.4700]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6337, 0.3663]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4908, 0.5092]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4869, 0.5131]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6220, 0.3780]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6437, 0.3563]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2853, 0.7147]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6442, 0.3558]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6108, 0.3892]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5838, 0.4162]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6521, 0.3479]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3846, 0.6154]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5065, 0.4935]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5198, 0.4802]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5953, 0.4047]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3271, 0.6729]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7484, 0.2516]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5419, 0.4581]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5686, 0.4314]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7482, 0.2518]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5166, 0.4834]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4621, 0.5379]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4969, 0.5031]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3701, 0.6299]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5816, 0.4184]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3698, 0.6302]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8007, 0.1993]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6427, 0.3573]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6409, 0.3591]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5366, 0.4634]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3941, 0.6059]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5133, 0.4867]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6208, 0.3792]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6142, 0.3858]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3798, 0.6202]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4817, 0.5183]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5810, 0.4190]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3322, 0.6678]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5355, 0.4645]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4853, 0.5147]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5994, 0.4006]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3307, 0.6693]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6055, 0.3945]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4989, 0.5011]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5895, 0.4105]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5048, 0.4952]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3322, 0.6678]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3824, 0.6176]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4913, 0.5087]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5569, 0.4431]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4912, 0.5088]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5414, 0.4586]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6358, 0.3642]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5621, 0.4379]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3209, 0.6791]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5319, 0.4681]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6327, 0.3673]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3336, 0.6664]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7144, 0.2856]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6441, 0.3559]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2893, 0.7107]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6566, 0.3434]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5335, 0.4665]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4976, 0.5024]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5677, 0.4323]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5478, 0.4522]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4239, 0.5761]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5593, 0.4407]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5177, 0.4823]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5488, 0.4512]], grad_fn=<SoftmaxBackward>)\n",
      "Training [20%]\tLoss: -0.5575\n",
      "tensor([[0.5532, 0.4468]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6641, 0.3359]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6941, 0.3059]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6064, 0.3936]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6111, 0.3889]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6709, 0.3291]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7245, 0.2755]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3283, 0.6717]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3306, 0.6694]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5920, 0.4080]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3330, 0.6670]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5978, 0.4022]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4619, 0.5381]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3272, 0.6728]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5254, 0.4746]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3367, 0.6633]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6321, 0.3679]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5326, 0.4674]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5907, 0.4093]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2669, 0.7331]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5801, 0.4199]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6482, 0.3518]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6739, 0.3261]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4158, 0.5842]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3068, 0.6932]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5077, 0.4923]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4936, 0.5064]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5852, 0.4148]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5290, 0.4710]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4774, 0.5226]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6605, 0.3395]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4850, 0.5150]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5074, 0.4926]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7184, 0.2816]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5007, 0.4993]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6020, 0.3980]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6822, 0.3178]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6421, 0.3579]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1456, 0.8544]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3152, 0.6848]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6891, 0.3109]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5171, 0.4829]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5335, 0.4665]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6986, 0.3014]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4961, 0.5039]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6021, 0.3979]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6351, 0.3649]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1725, 0.8275]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5258, 0.4742]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2675, 0.7325]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4983, 0.5017]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5142, 0.4858]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4836, 0.5164]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3726, 0.6274]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3416, 0.6584]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5502, 0.4498]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5355, 0.4645]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5369, 0.4631]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6252, 0.3748]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6645, 0.3355]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6290, 0.3710]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4924, 0.5076]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6605, 0.3395]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4984, 0.5016]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6353, 0.3647]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6344, 0.3656]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5943, 0.4057]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6123, 0.3877]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4982, 0.5018]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4916, 0.5084]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3244, 0.6756]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3450, 0.6550]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5081, 0.4919]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6102, 0.3898]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6695, 0.3305]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6872, 0.3128]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6571, 0.3429]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6923, 0.3077]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6525, 0.3475]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6657, 0.3343]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5930, 0.4070]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5402, 0.4598]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5055, 0.4945]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4384, 0.5616]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6152, 0.3848]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5429, 0.4571]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4087, 0.5913]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5169, 0.4831]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4091, 0.5909]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5242, 0.4758]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5235, 0.4765]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5180, 0.4820]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3852, 0.6148]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4789, 0.5211]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5568, 0.4432]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5006, 0.4994]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7239, 0.2761]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3058, 0.6942]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3503, 0.6497]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6877, 0.3123]], grad_fn=<SoftmaxBackward>)\n",
      "Training [25%]\tLoss: -0.5450\n",
      "tensor([[0.6762, 0.3238]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6916, 0.3084]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4582, 0.5418]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5054, 0.4946]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5148, 0.4852]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8091, 0.1909]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5226, 0.4774]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4497, 0.5503]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5595, 0.4405]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4257, 0.5743]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5319, 0.4681]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7864, 0.2136]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4284, 0.5716]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6235, 0.3765]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4401, 0.5599]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3445, 0.6555]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7722, 0.2278]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6273, 0.3727]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5138, 0.4862]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4390, 0.5610]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4895, 0.5105]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3297, 0.6703]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4439, 0.5561]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5787, 0.4213]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4425, 0.5575]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4495, 0.5505]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5326, 0.4674]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1337, 0.8663]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5002, 0.4998]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5712, 0.4288]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7554, 0.2446]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6538, 0.3462]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4954, 0.5046]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8962, 0.1038]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5800, 0.4200]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3035, 0.6965]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2811, 0.7189]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4393, 0.5607]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5986, 0.4014]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6408, 0.3592]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3729, 0.6271]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5034, 0.4966]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5920, 0.4080]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4617, 0.5383]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5674, 0.4326]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5156, 0.4844]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5284, 0.4716]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8334, 0.1666]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3352, 0.6648]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5256, 0.4744]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3975, 0.6025]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6367, 0.3633]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5941, 0.4059]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6635, 0.3365]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6390, 0.3610]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5023, 0.4977]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6774, 0.3226]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5947, 0.4053]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5333, 0.4667]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4857, 0.5143]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4836, 0.5164]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4224, 0.5776]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5934, 0.4066]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6693, 0.3307]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8906, 0.1094]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6582, 0.3418]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4021, 0.5979]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3434, 0.6566]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3766, 0.6234]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4066, 0.5934]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6575, 0.3425]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3262, 0.6738]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5922, 0.4078]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5118, 0.4882]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4661, 0.5339]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1629, 0.8371]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6099, 0.3901]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5006, 0.4994]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6277, 0.3723]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6184, 0.3816]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2863, 0.7137]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5770, 0.4230]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6309, 0.3691]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4544, 0.5456]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6512, 0.3488]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5157, 0.4843]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7590, 0.2410]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4059, 0.5941]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5134, 0.4866]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4760, 0.5240]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7431, 0.2569]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6335, 0.3665]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5363, 0.4637]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5458, 0.4542]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3247, 0.6753]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5193, 0.4807]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5610, 0.4390]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7433, 0.2567]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3202, 0.6798]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5998, 0.4002]], grad_fn=<SoftmaxBackward>)\n",
      "Training [30%]\tLoss: -0.5241\n",
      "tensor([[0.2854, 0.7146]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6110, 0.3890]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6646, 0.3354]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6475, 0.3525]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3575, 0.6425]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6394, 0.3606]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4562, 0.5438]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3974, 0.6026]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6253, 0.3747]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6232, 0.3768]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3263, 0.6737]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6419, 0.3581]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2320, 0.7680]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4819, 0.5181]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6758, 0.3242]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4404, 0.5596]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5680, 0.4320]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3543, 0.6457]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3997, 0.6003]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5698, 0.4302]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7753, 0.2247]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6642, 0.3358]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4920, 0.5080]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3787, 0.6213]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4772, 0.5228]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3150, 0.6850]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5626, 0.4374]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4088, 0.5912]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5901, 0.4099]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6113, 0.3887]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6210, 0.3790]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1877, 0.8123]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2668, 0.7332]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4934, 0.5066]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3541, 0.6459]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8107, 0.1893]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5034, 0.4966]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6516, 0.3484]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4212, 0.5788]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7289, 0.2711]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5260, 0.4740]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3799, 0.6201]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6015, 0.3985]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5400, 0.4600]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6461, 0.3539]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6891, 0.3109]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5643, 0.4357]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5472, 0.4528]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3386, 0.6614]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5645, 0.4355]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3993, 0.6007]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5262, 0.4738]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6944, 0.3056]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5059, 0.4941]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7786, 0.2214]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6681, 0.3319]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5077, 0.4923]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6293, 0.3707]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5461, 0.4539]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4422, 0.5578]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5043, 0.4957]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7071, 0.2929]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8442, 0.1558]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6375, 0.3625]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8694, 0.1306]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1959, 0.8041]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3126, 0.6874]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5175, 0.4825]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5526, 0.4474]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3390, 0.6610]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2057, 0.7943]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3512, 0.6488]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4701, 0.5299]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3776, 0.6224]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6172, 0.3828]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5712, 0.4288]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3240, 0.6760]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4968, 0.5032]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5119, 0.4881]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4878, 0.5122]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3749, 0.6251]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3990, 0.6010]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3941, 0.6059]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4633, 0.5367]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3781, 0.6219]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4758, 0.5242]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6336, 0.3664]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7418, 0.2582]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2606, 0.7394]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5463, 0.4537]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5002, 0.4998]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2166, 0.7834]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6642, 0.3358]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5284, 0.4716]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5769, 0.4231]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6365, 0.3635]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3610, 0.6390]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4814, 0.5186]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4614, 0.5386]], grad_fn=<SoftmaxBackward>)\n",
      "Training [35%]\tLoss: -0.5197\n",
      "tensor([[0.8247, 0.1753]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1582, 0.8418]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4796, 0.5204]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6209, 0.3791]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4796, 0.5204]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6368, 0.3632]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5238, 0.4762]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6365, 0.3635]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5353, 0.4647]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5177, 0.4823]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4782, 0.5218]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7733, 0.2267]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4049, 0.5951]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4843, 0.5157]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5536, 0.4464]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5120, 0.4880]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3684, 0.6316]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5091, 0.4909]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4692, 0.5308]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4993, 0.5007]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2516, 0.7484]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4748, 0.5252]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4774, 0.5226]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6537, 0.3463]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6860, 0.3140]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5583, 0.4417]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5660, 0.4340]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5916, 0.4084]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4531, 0.5469]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5780, 0.4220]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6294, 0.3706]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3479, 0.6521]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4767, 0.5233]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4876, 0.5124]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6196, 0.3804]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4968, 0.5032]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4795, 0.5205]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6367, 0.3633]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3620, 0.6380]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5342, 0.4658]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5005, 0.4995]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6664, 0.3336]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2755, 0.7245]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4859, 0.5141]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4450, 0.5550]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5556, 0.4444]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3680, 0.6320]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3445, 0.6555]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4860, 0.5140]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5426, 0.4574]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3649, 0.6351]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5590, 0.4410]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4415, 0.5585]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3172, 0.6828]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4848, 0.5152]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6122, 0.3878]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2090, 0.7910]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5852, 0.4148]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5091, 0.4909]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6377, 0.3623]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5063, 0.4937]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3721, 0.6279]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7012, 0.2988]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5029, 0.4971]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8030, 0.1970]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4834, 0.5166]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6221, 0.3779]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4834, 0.5166]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6028, 0.3972]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5525, 0.4475]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1824, 0.8176]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5260, 0.4740]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4764, 0.5236]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7543, 0.2457]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6890, 0.3110]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4400, 0.5600]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4975, 0.5025]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3273, 0.6727]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2379, 0.7621]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4707, 0.5293]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3706, 0.6294]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7200, 0.2800]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4295, 0.5705]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6994, 0.3006]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4669, 0.5331]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3864, 0.6136]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2898, 0.7102]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3469, 0.6531]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4904, 0.5096]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5949, 0.4051]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4802, 0.5198]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4690, 0.5310]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3083, 0.6917]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5576, 0.4424]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6207, 0.3793]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4720, 0.5280]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4696, 0.5304]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6811, 0.3189]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4176, 0.5824]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3717, 0.6283]], grad_fn=<SoftmaxBackward>)\n",
      "Training [40%]\tLoss: -0.5087\n",
      "tensor([[0.4418, 0.5582]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5614, 0.4386]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5103, 0.4897]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6126, 0.3874]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4731, 0.5269]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6066, 0.3934]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5151, 0.4849]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4572, 0.5428]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5753, 0.4247]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5003, 0.4997]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4610, 0.5390]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4550, 0.5450]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2472, 0.7528]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5887, 0.4113]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4707, 0.5293]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5682, 0.4318]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4574, 0.5426]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2098, 0.7902]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5850, 0.4150]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5410, 0.4590]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5223, 0.4777]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3200, 0.6800]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5897, 0.4103]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3783, 0.6217]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3284, 0.6716]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2262, 0.7738]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5565, 0.4435]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4114, 0.5886]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3323, 0.6677]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6774, 0.3226]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4835, 0.5165]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3419, 0.6581]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5093, 0.4907]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4698, 0.5302]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4720, 0.5280]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6210, 0.3790]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5448, 0.4552]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5974, 0.4026]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4953, 0.5047]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5042, 0.4958]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7264, 0.2736]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2343, 0.7657]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5081, 0.4919]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6380, 0.3620]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4393, 0.5607]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5002, 0.4998]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4774, 0.5226]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3679, 0.6321]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5230, 0.4770]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4026, 0.5974]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4845, 0.5155]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3026, 0.6974]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2628, 0.7372]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7414, 0.2586]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4836, 0.5164]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3984, 0.6016]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4861, 0.5139]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4109, 0.5891]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5518, 0.4482]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5672, 0.4328]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2926, 0.7074]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2914, 0.7086]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3790, 0.6210]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5796, 0.4204]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5856, 0.4144]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2165, 0.7835]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4826, 0.5174]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4954, 0.5046]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5275, 0.4725]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3459, 0.6541]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4999, 0.5001]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4869, 0.5131]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4167, 0.5833]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4999, 0.5001]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4443, 0.5557]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4850, 0.5150]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6169, 0.3831]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3588, 0.6412]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3601, 0.6399]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4778, 0.5222]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3716, 0.6284]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4819, 0.5181]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5694, 0.4306]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2895, 0.7105]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4786, 0.5214]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4907, 0.5093]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3523, 0.6477]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4344, 0.5656]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2167, 0.7833]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5149, 0.4851]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4497, 0.5503]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4787, 0.5213]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2422, 0.7578]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3512, 0.6488]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4717, 0.5283]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4046, 0.5954]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5963, 0.4037]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5686, 0.4314]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3393, 0.6607]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4795, 0.5205]], grad_fn=<SoftmaxBackward>)\n",
      "Training [45%]\tLoss: -0.4733\n",
      "tensor([[0.6099, 0.3901]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5998, 0.4002]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6939, 0.3061]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3171, 0.6829]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3322, 0.6678]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3414, 0.6586]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4274, 0.5726]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3894, 0.6106]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4397, 0.5603]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5984, 0.4016]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5060, 0.4940]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4030, 0.5970]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4577, 0.5423]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6217, 0.3783]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6800, 0.3200]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5437, 0.4563]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4936, 0.5064]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3327, 0.6673]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4938, 0.5062]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5497, 0.4503]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2707, 0.7293]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5545, 0.4455]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5800, 0.4200]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4898, 0.5102]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5947, 0.4053]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3424, 0.6576]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5758, 0.4242]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6142, 0.3858]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6780, 0.3220]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4943, 0.5057]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4948, 0.5052]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6422, 0.3578]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5766, 0.4234]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4257, 0.5743]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5900, 0.4100]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5856, 0.4144]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5465, 0.4535]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2159, 0.7841]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5051, 0.4949]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3567, 0.6433]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4235, 0.5765]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4545, 0.5455]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4868, 0.5132]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6327, 0.3673]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5255, 0.4745]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4844, 0.5156]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4937, 0.5063]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5921, 0.4079]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5382, 0.4618]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5981, 0.4019]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3236, 0.6764]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7215, 0.2785]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3797, 0.6203]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4012, 0.5988]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6414, 0.3586]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5665, 0.4335]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4045, 0.5955]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5637, 0.4363]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5066, 0.4934]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2905, 0.7095]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4902, 0.5098]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4004, 0.5996]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6216, 0.3784]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4783, 0.5217]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5916, 0.4084]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4567, 0.5433]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4849, 0.5151]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3135, 0.6865]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1844, 0.8156]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5144, 0.4856]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5198, 0.4802]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4093, 0.5907]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3363, 0.6637]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6224, 0.3776]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3403, 0.6597]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3044, 0.6956]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5172, 0.4828]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5482, 0.4518]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4846, 0.5154]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4827, 0.5173]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4492, 0.5508]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4716, 0.5284]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5020, 0.4980]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3560, 0.6440]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4659, 0.5341]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4629, 0.5371]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5019, 0.4981]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5514, 0.4486]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4642, 0.5358]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5872, 0.4128]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5312, 0.4688]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4565, 0.5435]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4056, 0.5944]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5244, 0.4756]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6009, 0.3991]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4539, 0.5461]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4585, 0.5415]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7651, 0.2349]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6758, 0.3242]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3393, 0.6607]], grad_fn=<SoftmaxBackward>)\n",
      "Training [50%]\tLoss: -0.5060\n",
      "tensor([[0.4570, 0.5430]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3163, 0.6837]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7002, 0.2998]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7220, 0.2780]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3248, 0.6752]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5413, 0.4587]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4640, 0.5360]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3054, 0.6946]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1748, 0.8252]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4245, 0.5755]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4064, 0.5936]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6091, 0.3909]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3267, 0.6733]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5691, 0.4309]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2489, 0.7511]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7218, 0.2782]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6002, 0.3998]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4528, 0.5472]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3054, 0.6946]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4291, 0.5709]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4957, 0.5043]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3154, 0.6846]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4858, 0.5142]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4375, 0.5625]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3973, 0.6027]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4480, 0.5520]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2970, 0.7030]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2845, 0.7155]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5339, 0.4661]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5895, 0.4105]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3049, 0.6951]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3107, 0.6893]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2934, 0.7066]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2196, 0.7804]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5290, 0.4710]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5604, 0.4396]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3006, 0.6994]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2121, 0.7879]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4382, 0.5618]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4722, 0.5278]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4808, 0.5192]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5669, 0.4331]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6871, 0.3129]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5047, 0.4953]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2757, 0.7243]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2947, 0.7053]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4407, 0.5593]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1579, 0.8421]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3189, 0.6811]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3925, 0.6075]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5655, 0.4345]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3914, 0.6086]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1047, 0.8953]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4471, 0.5529]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3821, 0.6179]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2092, 0.7908]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4474, 0.5526]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3264, 0.6736]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7257, 0.2743]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5074, 0.4926]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6247, 0.3753]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2347, 0.7653]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5902, 0.4098]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5693, 0.4307]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3971, 0.6029]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4698, 0.5302]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2940, 0.7060]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2782, 0.7218]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3573, 0.6427]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3128, 0.6872]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5327, 0.4673]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3538, 0.6462]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5884, 0.4116]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5328, 0.4672]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5934, 0.4066]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4525, 0.5475]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7359, 0.2641]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1486, 0.8514]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4166, 0.5834]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3366, 0.6634]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6534, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4380, 0.5620]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1892, 0.8108]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3983, 0.6017]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3639, 0.6361]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2577, 0.7423]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4627, 0.5373]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2514, 0.7486]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6037, 0.3963]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3789, 0.6211]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3490, 0.6510]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4313, 0.5687]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4367, 0.5633]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5994, 0.4006]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3299, 0.6701]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2686, 0.7314]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3065, 0.6935]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2735, 0.7265]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4383, 0.5617]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3003, 0.6997]], grad_fn=<SoftmaxBackward>)\n",
      "Training [55%]\tLoss: -0.5131\n",
      "tensor([[0.3174, 0.6826]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4370, 0.5630]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6221, 0.3779]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6086, 0.3914]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3352, 0.6648]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5768, 0.4232]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7476, 0.2524]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4744, 0.5256]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5762, 0.4238]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5242, 0.4758]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4425, 0.5575]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4079, 0.5921]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2839, 0.7161]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6244, 0.3756]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5539, 0.4461]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5539, 0.4461]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4730, 0.5270]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3473, 0.6527]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4308, 0.5692]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4720, 0.5280]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6061, 0.3939]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3677, 0.6323]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2026, 0.7974]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3886, 0.6114]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5017, 0.4983]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3879, 0.6121]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6048, 0.3952]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5201, 0.4799]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4596, 0.5404]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4709, 0.5291]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6669, 0.3331]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2781, 0.7219]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5947, 0.4053]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3159, 0.6841]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1648, 0.8352]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5842, 0.4158]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6140, 0.3860]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6113, 0.3887]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4989, 0.5011]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4701, 0.5299]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5027, 0.4973]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4029, 0.5971]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6238, 0.3762]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5469, 0.4531]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6100, 0.3900]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4796, 0.5204]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4680, 0.5320]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4773, 0.5227]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5271, 0.4729]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3064, 0.6936]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3920, 0.6080]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5125, 0.4875]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5826, 0.4174]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3857, 0.6143]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6043, 0.3957]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3176, 0.6824]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3368, 0.6632]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4655, 0.5345]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5821, 0.4179]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5693, 0.4307]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4346, 0.5654]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5122, 0.4878]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4804, 0.5196]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3071, 0.6929]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4044, 0.5956]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5859, 0.4141]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4939, 0.5061]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6518, 0.3482]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3865, 0.6135]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5482, 0.4518]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5469, 0.4531]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6334, 0.3666]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4459, 0.5541]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5844, 0.4156]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3468, 0.6532]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4905, 0.5095]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1901, 0.8099]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3209, 0.6791]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4769, 0.5231]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2487, 0.7513]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7612, 0.2388]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4316, 0.5684]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5708, 0.4292]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3553, 0.6447]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4049, 0.5951]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4811, 0.5189]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5624, 0.4376]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4500, 0.5500]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2886, 0.7114]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3649, 0.6351]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4661, 0.5339]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2332, 0.7668]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4474, 0.5526]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2733, 0.7267]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4084, 0.5916]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4022, 0.5978]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2817, 0.7183]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4722, 0.5278]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3962, 0.6038]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2714, 0.7286]], grad_fn=<SoftmaxBackward>)\n",
      "Training [60%]\tLoss: -0.4867\n",
      "tensor([[0.5783, 0.4217]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5374, 0.4626]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4556, 0.5444]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4667, 0.5333]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5202, 0.4798]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2854, 0.7146]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4450, 0.5550]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3349, 0.6651]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5870, 0.4130]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5598, 0.4402]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3829, 0.6171]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4406, 0.5594]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5840, 0.4160]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7579, 0.2421]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2999, 0.7001]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1704, 0.8296]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5210, 0.4790]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4506, 0.5494]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5053, 0.4947]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4728, 0.5272]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3552, 0.6448]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4956, 0.5044]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4604, 0.5396]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3209, 0.6791]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5807, 0.4193]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3199, 0.6801]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4625, 0.5375]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3536, 0.6464]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4114, 0.5886]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6095, 0.3905]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4123, 0.5877]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4347, 0.5653]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3086, 0.6914]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1919, 0.8081]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3056, 0.6944]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4517, 0.5483]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4029, 0.5971]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4807, 0.5193]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4276, 0.5724]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2767, 0.7233]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4453, 0.5547]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2732, 0.7268]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4870, 0.5130]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2846, 0.7154]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2639, 0.7361]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5435, 0.4565]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3949, 0.6051]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5479, 0.4521]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4425, 0.5575]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4320, 0.5680]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5066, 0.4934]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4980, 0.5020]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3738, 0.6262]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3796, 0.6204]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3362, 0.6638]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4527, 0.5473]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4786, 0.5214]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5808, 0.4192]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5831, 0.4169]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5234, 0.4766]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4345, 0.5655]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2215, 0.7785]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4950, 0.5050]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3505, 0.6495]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2457, 0.7543]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3594, 0.6406]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4668, 0.5332]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6355, 0.3645]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3666, 0.6334]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5796, 0.4204]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4495, 0.5505]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3012, 0.6988]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6001, 0.3999]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4752, 0.5248]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5580, 0.4420]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5200, 0.4800]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5462, 0.4538]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4815, 0.5185]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5769, 0.4231]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4792, 0.5208]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4695, 0.5305]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4435, 0.5565]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5024, 0.4976]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3046, 0.6954]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3419, 0.6581]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3739, 0.6261]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3957, 0.6043]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4055, 0.5945]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5191, 0.4809]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4737, 0.5263]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5592, 0.4408]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4403, 0.5597]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5519, 0.4481]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2458, 0.7542]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3463, 0.6537]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4380, 0.5620]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4495, 0.5505]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3256, 0.6744]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4433, 0.5567]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4437, 0.5563]], grad_fn=<SoftmaxBackward>)\n",
      "Training [65%]\tLoss: -0.4815\n",
      "tensor([[0.4419, 0.5581]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4857, 0.5143]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4497, 0.5503]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4303, 0.5697]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4698, 0.5302]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5025, 0.4975]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4463, 0.5537]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4071, 0.5929]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4098, 0.5902]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4139, 0.5861]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4407, 0.5593]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4408, 0.5592]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3399, 0.6601]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5710, 0.4290]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5537, 0.4463]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4675, 0.5325]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5077, 0.4923]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4371, 0.5629]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4103, 0.5897]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4316, 0.5684]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5136, 0.4864]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5826, 0.4174]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3892, 0.6108]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5751, 0.4249]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4498, 0.5502]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4676, 0.5324]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3056, 0.6944]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4436, 0.5564]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4625, 0.5375]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4098, 0.5902]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4011, 0.5989]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4547, 0.5453]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3154, 0.6846]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4244, 0.5756]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4311, 0.5689]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5664, 0.4336]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4752, 0.5248]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4423, 0.5577]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4732, 0.5268]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4394, 0.5606]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5788, 0.4212]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5659, 0.4341]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4251, 0.5749]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3693, 0.6307]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4365, 0.5635]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5373, 0.4627]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4334, 0.5666]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3065, 0.6935]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4321, 0.5679]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4353, 0.5647]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3969, 0.6031]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4256, 0.5744]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4359, 0.5641]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5162, 0.4838]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4312, 0.5688]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4327, 0.5673]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3330, 0.6670]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3900, 0.6100]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4512, 0.5488]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3777, 0.6223]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3384, 0.6616]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3506, 0.6494]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5349, 0.4651]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4425, 0.5575]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4821, 0.5179]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4106, 0.5894]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4236, 0.5764]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4678, 0.5322]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4483, 0.5517]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3433, 0.6567]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2058, 0.7942]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2136, 0.7864]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5110, 0.4890]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4315, 0.5685]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5073, 0.4927]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5404, 0.4596]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3218, 0.6782]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4417, 0.5583]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2477, 0.7523]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5589, 0.4411]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4698, 0.5302]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5506, 0.4494]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2285, 0.7715]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4531, 0.5469]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3007, 0.6993]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3329, 0.6671]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4689, 0.5311]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4814, 0.5186]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6896, 0.3104]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5134, 0.4866]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4715, 0.5285]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3123, 0.6877]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4446, 0.5554]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6211, 0.3789]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4437, 0.5563]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4467, 0.5533]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4027, 0.5973]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4453, 0.5547]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5594, 0.4406]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3689, 0.6311]], grad_fn=<SoftmaxBackward>)\n",
      "Training [70%]\tLoss: -0.4903\n",
      "tensor([[0.4330, 0.5670]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4731, 0.5269]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3939, 0.6061]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5464, 0.4536]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3354, 0.6646]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5514, 0.4486]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4598, 0.5402]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5410, 0.4590]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4856, 0.5144]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5113, 0.4887]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1911, 0.8089]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3713, 0.6287]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4387, 0.5613]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5545, 0.4455]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4694, 0.5306]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5454, 0.4546]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4387, 0.5613]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4547, 0.5453]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5235, 0.4765]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4780, 0.5220]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4510, 0.5490]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4617, 0.5383]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3525, 0.6475]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4713, 0.5287]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4479, 0.5521]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4075, 0.5925]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3987, 0.6013]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5427, 0.4573]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4519, 0.5481]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6427, 0.3573]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4007, 0.5993]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4191, 0.5809]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2617, 0.7383]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4587, 0.5413]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5645, 0.4355]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5056, 0.4944]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4534, 0.5466]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2772, 0.7228]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4704, 0.5296]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3710, 0.6290]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3972, 0.6028]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4621, 0.5379]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5033, 0.4967]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4842, 0.5158]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5628, 0.4372]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3117, 0.6883]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5831, 0.4169]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4828, 0.5172]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3751, 0.6249]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2947, 0.7053]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4436, 0.5564]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4484, 0.5516]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5561, 0.4439]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5630, 0.4370]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3932, 0.6068]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5555, 0.4445]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3105, 0.6895]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4394, 0.5606]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4477, 0.5523]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3889, 0.6111]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4500, 0.5500]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4586, 0.5414]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4581, 0.5419]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4878, 0.5122]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2411, 0.7589]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4081, 0.5919]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4324, 0.5676]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4326, 0.5674]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4189, 0.5811]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4488, 0.5512]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4771, 0.5229]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4350, 0.5650]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5192, 0.4808]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3040, 0.6960]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4408, 0.5592]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4308, 0.5692]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5739, 0.4261]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3339, 0.6661]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3365, 0.6635]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3585, 0.6415]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4566, 0.5434]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4238, 0.5762]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2051, 0.7949]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5536, 0.4464]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6369, 0.3631]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4274, 0.5726]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7037, 0.2963]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3415, 0.6585]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4524, 0.5476]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5640, 0.4360]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4483, 0.5517]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4638, 0.5362]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4363, 0.5637]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4693, 0.5307]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6670, 0.3330]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4336, 0.5664]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4401, 0.5599]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3479, 0.6521]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2239, 0.7761]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2596, 0.7404]], grad_fn=<SoftmaxBackward>)\n",
      "Training [75%]\tLoss: -0.4917\n",
      "tensor([[0.4725, 0.5275]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4300, 0.5700]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3012, 0.6988]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4395, 0.5605]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4318, 0.5682]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3513, 0.6487]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5351, 0.4649]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4323, 0.5677]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3142, 0.6858]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4549, 0.5451]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4322, 0.5678]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4331, 0.5669]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5510, 0.4490]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4977, 0.5023]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4141, 0.5859]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4431, 0.5569]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4278, 0.5722]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3998, 0.6002]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4576, 0.5424]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4565, 0.5435]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3933, 0.6067]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4417, 0.5583]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4432, 0.5568]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4541, 0.5459]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5382, 0.4618]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3566, 0.6434]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4496, 0.5504]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4459, 0.5541]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4783, 0.5217]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5065, 0.4935]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2168, 0.7832]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3153, 0.6847]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4328, 0.5672]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3583, 0.6417]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4209, 0.5791]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4364, 0.5636]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5499, 0.4501]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4245, 0.5755]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4390, 0.5610]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5440, 0.4560]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4489, 0.5511]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4390, 0.5610]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2301, 0.7699]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4999, 0.5001]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4303, 0.5697]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5416, 0.4584]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4345, 0.5655]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3211, 0.6789]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4887, 0.5113]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3810, 0.6190]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4559, 0.5441]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3319, 0.6681]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5119, 0.4881]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5182, 0.4818]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5454, 0.4546]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5242, 0.4758]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4384, 0.5616]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4383, 0.5617]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4526, 0.5474]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4398, 0.5602]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4692, 0.5308]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4743, 0.5257]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3531, 0.6469]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3870, 0.6130]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4352, 0.5648]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5083, 0.4917]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4036, 0.5964]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4611, 0.5389]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4786, 0.5214]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4770, 0.5230]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4447, 0.5553]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7898, 0.2102]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4600, 0.5400]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4321, 0.5679]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4802, 0.5198]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4877, 0.5123]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2629, 0.7371]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4548, 0.5452]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4020, 0.5980]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4381, 0.5619]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5119, 0.4881]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3296, 0.6704]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4977, 0.5023]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4326, 0.5674]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4000, 0.6000]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4405, 0.5595]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4527, 0.5473]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3803, 0.6197]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7490, 0.2510]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2675, 0.7325]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3616, 0.6384]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5324, 0.4676]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4542, 0.5458]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5339, 0.4661]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4391, 0.5609]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3225, 0.6775]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4449, 0.5551]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5528, 0.4472]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4802, 0.5198]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4999, 0.5001]], grad_fn=<SoftmaxBackward>)\n",
      "Training [80%]\tLoss: -0.5005\n",
      "tensor([[0.4300, 0.5700]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3113, 0.6887]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5678, 0.4322]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4326, 0.5674]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4817, 0.5183]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2999, 0.7001]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4081, 0.5919]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2143, 0.7857]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4331, 0.5669]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4267, 0.5733]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4235, 0.5765]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2954, 0.7046]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4425, 0.5575]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4370, 0.5630]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2923, 0.7077]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4020, 0.5980]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4523, 0.5477]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3888, 0.6112]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5786, 0.4214]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4307, 0.5693]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4298, 0.5702]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3166, 0.6834]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5751, 0.4249]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5352, 0.4648]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4675, 0.5325]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4381, 0.5619]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4294, 0.5706]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4897, 0.5103]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5469, 0.4531]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4322, 0.5678]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4583, 0.5417]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7166, 0.2834]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4425, 0.5575]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4330, 0.5670]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5702, 0.4298]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4924, 0.5076]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5771, 0.4229]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4340, 0.5660]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4679, 0.5321]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7042, 0.2958]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1313, 0.8687]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4235, 0.5765]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3077, 0.6923]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4259, 0.5741]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2697, 0.7303]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4269, 0.5731]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4843, 0.5157]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4001, 0.5999]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4180, 0.5820]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5132, 0.4868]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3004, 0.6996]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4245, 0.5755]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3043, 0.6957]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5438, 0.4562]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4447, 0.5553]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4635, 0.5365]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4143, 0.5857]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2552, 0.7448]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3751, 0.6249]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4264, 0.5736]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4379, 0.5621]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2017, 0.7983]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4435, 0.5565]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4198, 0.5802]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4341, 0.5659]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3681, 0.6319]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4723, 0.5277]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2656, 0.7344]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4247, 0.5753]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4306, 0.5694]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6958, 0.3042]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5009, 0.4991]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4658, 0.5342]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6708, 0.3292]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5247, 0.4753]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2640, 0.7360]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4290, 0.5710]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7023, 0.2977]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2046, 0.7954]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4094, 0.5906]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5583, 0.4417]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4316, 0.5684]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4803, 0.5197]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4887, 0.5113]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6336, 0.3664]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4242, 0.5758]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2216, 0.7784]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4305, 0.5695]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5659, 0.4341]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2961, 0.7039]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5152, 0.4848]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5846, 0.4154]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4709, 0.5291]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4835, 0.5165]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5509, 0.4491]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4487, 0.5513]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5299, 0.4701]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5211, 0.4789]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5445, 0.4555]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5210, 0.4790]], grad_fn=<SoftmaxBackward>)\n",
      "Training [85%]\tLoss: -0.4940\n",
      "tensor([[0.4481, 0.5519]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3959, 0.6041]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3006, 0.6994]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1519, 0.8481]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4283, 0.5717]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3504, 0.6496]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2190, 0.7810]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4039, 0.5961]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6811, 0.3189]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4343, 0.5657]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5294, 0.4706]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4585, 0.5415]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4177, 0.5823]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4283, 0.5717]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3440, 0.6560]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5504, 0.4496]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4220, 0.5780]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4181, 0.5819]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4728, 0.5272]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4455, 0.5545]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3988, 0.6012]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4146, 0.5854]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5518, 0.4482]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2779, 0.7221]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3629, 0.6371]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4183, 0.5817]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4190, 0.5810]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1923, 0.8077]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3128, 0.6872]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3194, 0.6806]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4099, 0.5901]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3803, 0.6197]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4180, 0.5820]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4435, 0.5565]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4140, 0.5860]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4696, 0.5304]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6666, 0.3334]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5253, 0.4747]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4219, 0.5781]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4267, 0.5733]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1359, 0.8641]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5519, 0.4481]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4098, 0.5902]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4215, 0.5785]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4411, 0.5589]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4948, 0.5052]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4291, 0.5709]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3340, 0.6660]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3023, 0.6977]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4712, 0.5288]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4479, 0.5521]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5097, 0.4903]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4685, 0.5315]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4303, 0.5697]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5353, 0.4647]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4309, 0.5691]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3302, 0.6698]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4343, 0.5657]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4338, 0.5662]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7484, 0.2516]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4467, 0.5533]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4384, 0.5616]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5405, 0.4595]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4469, 0.5531]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4371, 0.5629]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4180, 0.5820]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5269, 0.4731]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4476, 0.5524]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4957, 0.5043]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5783, 0.4217]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5084, 0.4916]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4920, 0.5080]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2101, 0.7899]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3620, 0.6380]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4329, 0.5671]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4332, 0.5668]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3390, 0.6610]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3841, 0.6159]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4512, 0.5488]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4583, 0.5417]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4360, 0.5640]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4575, 0.5425]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4302, 0.5698]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3154, 0.6846]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3864, 0.6136]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4532, 0.5468]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4367, 0.5633]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4125, 0.5875]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4646, 0.5354]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3050, 0.6950]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4414, 0.5586]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4477, 0.5523]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4523, 0.5477]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5237, 0.4763]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4510, 0.5490]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7535, 0.2465]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5552, 0.4448]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4423, 0.5577]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4625, 0.5375]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7197, 0.2803]], grad_fn=<SoftmaxBackward>)\n",
      "Training [90%]\tLoss: -0.4892\n",
      "tensor([[0.8015, 0.1985]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2038, 0.7962]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4389, 0.5611]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4150, 0.5850]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7954, 0.2046]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4109, 0.5891]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4295, 0.5705]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5474, 0.4526]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4346, 0.5654]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4342, 0.5658]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4524, 0.5476]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4395, 0.5605]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4430, 0.5570]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4768, 0.5232]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4595, 0.5405]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4426, 0.5574]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4383, 0.5617]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4265, 0.5735]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         print(batch_idx)\n",
    "        optimizer.zero_grad()        \n",
    "        # Forward pass\n",
    "        output = network(data)\n",
    "        # Calculating loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "    print('Training [{:.0f}%]\\tLoss: {:.4f}'.format(\n",
    "        100. * (epoch + 1) / epochs, loss_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Neg Log Likelihood Loss')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.title('Hybrid NN Training Convergence')\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Neg Log Likelihood Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy of NN\n",
    "\n",
    "The outcome is not always the same because the prediction is probabilistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data is is: 0.61\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "number = 0\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    number +=1\n",
    "    output = network(data)\n",
    "    output = (output>0.5).float()\n",
    "    accuracy += (output[0][1].item() == target[0].item())*1\n",
    "    \n",
    "print(\"Performance on test data is is: {}\".format(accuracy/number))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAABxCAYAAAA6YcICAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAARWElEQVR4nO3de4xVVX/G8efHyN0gviLgBUYRTaGjhlcLBFHxUkUJkSgXSVDBCyKSCuS1RaUGwSYai4wXKpq+qRXEKNAa8QJSlUEREU29QEWkKCKoFAR5QUHQ3T/mnOWa4ZyZM8w5e8856/tJTJ7ZZ7P34jebmeVae+1tURQJAAAgFM2SbgAAAECc6PwAAICg0PkBAABBofMDAACCQucHAAAEhc4PAAAISpPv/JjZKWYWmdlRqa9fM7MbYjjvNDObV+jzNHXUP1nUPznUPlnUP1mlXv+8dH7M7Csz+9nM9prZ92b2b2Z2dD6OXVsURVdEUfTvObbp0kK0wcxamNnC1DkiMxtQiPM0oD1B1T91/EvMbL2Z/WRmb5lZeaHOlUNbgqp/U7r+Q6t96vhc+/W3qeSv/VR7qP8RyufIz+Aoio6W9EdJfyNpau0drFqTH23K0TuSRkn6LumGpARTfzPrIOk/JP2jpD9I+kDS84k2KqD6pzSl6z+Y2nPtNwlN6dqXqP8RyXsxoijaKuk1SRWSZGbLzeyfzGylpJ8kdTOzY8zsz2b2rZltNbP7zawstX+Zmf2zme0ws02SBvnHTx3vZu/rW8zsMzP7i5n9j5n90czmSuoqaXGqR/z3qX37mtm7ZrbbzD72e41mdqqZVaWOs0xShzr+jr9EUVQZRdE7kn7NT+XyI4T6S7pa0rooihZEUbRf0jRJZ5vZXzW2fo0VQv2b6vUfQu3Ftc+1nwX1b3jBGv2fpK8kXZrKXSStkzQj9fVySV9L+mtJR0lqLulFSU9Kaiupo6T3Jd2a2n+cpPWp4/xB0luSIklHece7OZWHSdqq6t6uSeouqbx2m1JfnyRpp6QrVd3p+9vU18enPl8l6WFJLSVdIOkvkubl8Hf/RtKAfNSR+udWf0mPSHqi1ra1kq6h/mFd/6HVnmufa5/656f++fwG7JW0W9JmSf8iqbVXsOnevp0kHUh/nto2UtJbqfympHHeZ5fV8Q1YKumO+i6K1Nf/IGlurX2WSrpB1T3VQ5Laep/NL7J/AMHUX9KfJT1Qa9tKSaOpf1jXf2i159rn2qf++an/UcqfIVEU/VeWz7Z4uVzVPdBvzSy9rZm3z4m19t9cxzm7SPrfHNtXLmmYmQ32tjVXde/2REm7oijaV+u8XXI8dlMQUv33SmpXa1s7Vf8fQ1JCqn9TE1Ltufarce3/jvofgXx2fuoSeXmLqnufHaIoOpRh329V8y/etY7jbpF0Wg7nTO87N4qiW2rvaNWrJY41s7beN6FrhmMUq1Kr/zpV/19D+s+3TbVjXR1tTVKp1b+YlFrtufZ/PxbXfv2ofxax3/0dRdG3kl6XNNPM2plZMzM7zcwuTO3ygqS/M7OTzexYSVPqONy/SvqTmZ1j1brb78s+v5fUzdt3nqTBZnZ56sauVmY2wMxOjqJos6pXTdxn1Uvp+ksarDqYWUsza5X6skXqeFbXn2kKSqT+/ympwsyuSX0P7pX0SRRF6xtaj7iVSP2L8vovkdpz7Vfj2m8g6l9LHucdL83y2XKl5gm9bcdIekLVc3Y/SvpvSdemPjtK0ixV3xD1paTblWXeMfX1OEmfq3o4eK2kXqntV6n6Zq/dkv6U2tZHUpWkHyT9n6RXJHVNfdZN0tup4yyT9LjqmHdM/Z2jWv+dko96Uv+c6n+pqm/O+znVpkRqH3D9m8T1H2jtufa59ql/I+tvqYMBAAAEoVQeegQAAJATOj8AACAodH4AAEBQ6PwAAICg0PkBAABBadBDDs2MpWGNEEXRET8Lgto32o4oio4/0j9M/RuN+ieInz2J4tpPVsb6M/KDUNT1qHYUHvVHqLj2k5Wx/nR+AABAUOj8AACAoND5AQAAQaHzAwAAgkLnBwAABIXODwAACAqdHwAAEBQ6PwAAICgNesJzku644w6XKysrXe7Vq5fLH330UaxtQv2+++47lzt16iRJmjBhgts2e/bs2NsEAGiYvn37urxq1SqXf/vtN5dXr17t8qxZs1xesGBBgVvXcIz8AACAoND5AQAAQSmaaa9zzjnHZX+Y7Y033nD56quvdrmqqiqehuEw/fr1c7lt27Yup79v/vcP+fHSSy+5PHjwYJfHjx/v8hNPPBFrmyD17NnT5aVLl0qSTjzxxIz7lpWVxdKmYpCeIvdvcejRo4fLZ511lstmv7+zdfr06S4/+uijLu/cubMg7QzJxIkTXfZ/hvu5T58+Ls+fP9/loUOHSqo5Ffbee+8VpJ25YuQHAAAEhc4PAAAIStFMe1VUVGTc3r59e5enTJniMtNeyTn99NNdbtOmTYItKW0tWrRw2a+zPwztr5J89tlnJUl79uyJoXWQpLFjx7rcuXNnSUz75mLatGmSpOHDh2f8PIqijHnq1KkuDxkyxOUrrrhCkrRt27Z8NrPk+fUfNmyYy/5UY7Nmzerdnv6z6ekvSTrvvPNcTmIKjJEfAAAQFDo/AAAgKEUz7fXggw+67N/R3717d5c7duwYa5uAJLVq1crldu3aZdzHn4Js3bq1JKa9Cm3kyJEujxo16rDPd+/e7fKOHTtiaVOx6dq1a877+lNZn3zyicsDBw50+ZVXXpEkXX755W7b9u3bG9PEIPjT5v50rT+l1ZDt/rbnn3/e5REjRrgc1xQYIz8AACAodH4AAEBQimbayx8i84fW5s2b5/Kpp57qcv/+/V1+5513Ctw65OLQoUOSpI0bNybcktLgDyH/+uuvGfeZMWOGy7t27Sp4m0J11VVXuez/TMq0smvu3LkuT548ubANK1L+Qwzrc9ddd7n8/vvvu3zmmWcedrzbb7/dbbvvvvtcZgVeTenft/4Da/0abd261WV/RZg/ZTVp0iSXZ86cKanm7+4uXbq4fPLJJ+ej2Q3CyA8AAAgKnR8AABCUopn28q1Zs8blb775xmV/6MwfUkO8evXqlXH7qlWrJEnLli2Lszkly18R07t373r3+eWXXwreplDdfPPNSTehpGzatElS9vegHTx40OWvv/7a5Q0bNrjsP+Qw/TvDfwjiggULXF67dm0jW1z8+vbt63L6HV3Z3uGVbarL57/HK/199N8P5k+B+Q+qjAsjPwAAICh0fgAAQFCKctrLXy20c+dOl/1pr8suu8zl5557Lp6GBWzMmDEu+w958/mrK4Bid8opp7jcqVMnl/3hfN8XX3whiRVeufj+++/r/NyfUlmxYkXGffwpsE8//VRSzRVggwYNcplpr5rv7krfNuJfy415EOGiRYsk1Zwu829N8afD0vsWGiM/AAAgKEU58pOLSy65JOkmBOXGG290uUOHDhn34XlL+eU/RwPxGzx4sMv+Tf7ZbhKdM2dOPA0rUu3bt3f5ggsuqHPfhx9+uN7j7d271+UJEyZIkqqqqty2iy++2GX/9Umh8kdfMj33qDE3JadHitKLXqSaMzWVlZVHfOwjxcgPAAAICp0fAAAQlJKd9kLhnXHGGS77bw/3LV261OVsr2DAkWnevHm9++zbty+GloSjRYsWLmeb3vXdf//9Ls+ePbsgbSoVjz32mMvHH3/8YZ9v2bLF5QMHDjTo2Om3vfuLZfypNT9nu4G6FPmvjTIzl9M3Oi9cuNBty8eNyP45/PzCCy+47J/Tv8k63xj5AQAAQaHzAwAAgsK0FxqkTZs2Lt99990u+8PU27dvd/nee+91mTcnx4+plvzyXxfiX//Z7Nixw2X/lQyoVlZW5rJf2zR/qnz06NEu79mzp0HnSe//ww8/uG3du3d3+c4773Q5pGkvfwWXn9M/q/3nKcVxvkKcMxtGfgAAQFDo/AAAgKAw7YUG8R/pf91112Xc57XXXnP5gw8+KHSTgnLssce6fNFFF2Xcxx/a//nnnwveppBMnz693n38a37x4sWFbE7RGzBggMv9+/c/7PP0aykkafny5Y0+31NPPeVy7969XW7dunWjj12Msq2+Sr+iqKGvsajPtdde67L/qgv/NRr5Pmc2jPwAAICg0PkBAABBYdoLDXLTTTfVu8+6detiaEmYWrZs6bL/bhzfypUrXd68eXPB21Tqevbs6fL555/vcra3t/fp06fgbSoVQ4cOzbh9//79kqTrr78+r+fbtGmTyzx0Nfvqq8a8x+tIzp3ESmBGfgAAQFDo/AAAgKAw7YV6DRo0yOVx48Zl3Of11193+ZFHHil4m4C4jB071uXOnTu77A/Vz5kzJ9Y2lbp0bfM9hV5VVeWy/36wbt26udyxY0eX/Qe2loouXbpkzNlWfuVTtneJxbXCy8fIDwAACAqdHwAAEBSmvZBVelhy4MCBblurVq1c9of9Z8yY4fKhQ4diaB1QWBdeeKEkadSoUfXu+/LLLxe6OUHZtm1brOcrLy932X9468yZM2NtRxz69u3rsv+gx0Ku9kqf018J6Z+jsrIyr+fLBSM/AAAgKCU78tO2bVuXKyoqXF67dm0SzSlKxx13nCRp/PjxGT9fsmSJy++++24sbQLiMmLECEnSMccck/Hzjz/+2GWep5Q7/03u/s9m36JFiwreDv/m2zFjxrh82mmnFfzcSdqyZYvL/gibf/NzeqSmMd8Hf4Rp2LBhh50jjhus68LIDwAACAqdHwAAEJSSnfZq3769y/7br5n2yt2rr7562LatW7e6nO2ZP0jWnj17km5CSbj11lslZX/0/ooVK1xev359LG0qBf5rJVavXu1yv379Ym3HNddck3H7vn37Ym1H3Pxn6qxatcpl/3U5EydOlCR17drVbZs1a1bGY0yaNMllf6rLv7k5Pd3l/1vyXw8Tx+s0amPkBwAABIXODwAACErRT3t9+eWXLp999tkJtqT09OjR47BtP/74o8v+FBji4T96P5snn3wyhpaUvkxvbZ8/f77LkydPjrM5Jenpp5922Z8+KZRzzz3X5ZYtW7q8e/dul2fPnl3wdjQV2VZcpa/99CotSRo6dGjGff0pq/q2+/+m/N8fSfwuYeQHAAAEhc4PAAAIStFPe/lvUx4yZEiCLSkN/jCn/yqLtA0bNsTZHNQyevTopJtQ0m677TaX0ytT/BUq06ZNi7tJJW3jxo0ur1mzxuUbbrhBkvTMM8+4bY1ZUXfCCSdIqvnQPn/ay39I61dffXXE5yk2/gouf1orfc3701TZVmo1ZLu/bfjw4S7zVncAAIACo/MDAACCUvTTXgcPHsyYmzdvnkRzit7UqVNdTg9V/vTTT27bQw89FHubgLhceeWVSTchKPv373f5xRdfdHn69OmSpDfffNNt86dlPvvsM5d37drl8tFHH+2y/4C+BQsWSKr5ID9/5ap/+0RI/OmmhQsXupx+WKH/Li5/SivTyrC6tqdXcyU91eVj5AcAAASFzg8AAAhK0U97LV++3OUpU6a4PHPmzARaU/z8FRUVFRWSpGXLlrltSQ9VIrMPP/wwYwaKxQMPPOByeXm5JGns2LFu29tvv+2yv0rMX4F60kknuZzpobf+qq7Fixe77E/5hGrEiBEup6e9Vq5c6bblsqpr5MiRLvsPOUxPezWl3x+M/AAAgKDQ+QEAAEEp+mkvX2VlZcaM3PlDn2h6Vq9enXF7VVWVy/4KGqAY3XPPPZKkuXPnum3+zyb/AYW33HKLy59//rnLjz/+uMtLliyRVHP12IEDB/LY4tKSnp4qKytLuCWFw8gPAAAIivk3JdW7s1nuO+MwURRZ/XtlRu0b7cMois6tf7fMqH+jUf8E8bMnUVz7ycpYf0Z+AABAUOj8AACAoND5AQAAQaHzAwAAgkLnBwAABIXODwAACAqdHwAAEBQ6PwAAICgNfb3FDkmbC9GQAJQ38s9T+8ah/smi/smh9smi/snKWP8GPeEZAACg2DHtBQAAgkLnBwAABIXODwAACAqdHwAAEBQ6PwAAICh0fgAAQFDo/AAAgKDQ+QEAAEGh8wMAAILy/1dVWHj29RnMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x216 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples_show = 6\n",
    "count = 0\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(10, 3))\n",
    "\n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if count == n_samples_show:\n",
    "            break\n",
    "        output = network(data)\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "\n",
    "        axes[count].imshow(data[0].numpy().squeeze(), cmap='gray')\n",
    "\n",
    "        axes[count].set_xticks([])\n",
    "        axes[count].set_yticks([])\n",
    "        axes[count].set_title('Predicted {}'.format(pred.item()))\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
