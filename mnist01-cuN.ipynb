{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- coding: utf-8 --\n",
    "# This code is part of Qiskit.\n",
    "#\n",
    "# (C) Copyright IBM 2019.\n",
    "#\n",
    "# This code is licensed under the Apache License, Version 2.0. You may\n",
    "# obtain a copy of this license in the LICENSE.txt file in the root directory\n",
    "# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "#\n",
    "# Any modifications or derivative works of this code must retain this\n",
    "# copyright notice, and modified files need to carry a notice indicating\n",
    "# that they have been altered from the originals.\n",
    "#\n",
    "# Code adapted from QizGloria team, Qiskit Camp Europe 2019, updated by \n",
    "# Team Ube Pancake, Qiskit Summer Jam 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumRegister,QuantumCircuit,ClassicalRegister,execute\n",
    "from qiskit.circuit import Parameter,ControlledGate\n",
    "from qiskit import Aer\n",
    "import qiskit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "NUM_QUBITS = 4\n",
    "NUM_SHOTS = 10000\n",
    "SHIFT = 0.01\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "SIMULATOR = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to translate Q-Circuit parameters from pytorch back to QISKIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numbers(tensor_list):\n",
    "    num_list = []\n",
    "    for tensor in tensor_list:\n",
    "        num_list += [tensor.item()]\n",
    "    return num_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Contruct QuantumCircuit QFT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T16:09:30.598730Z",
     "start_time": "2019-10-01T16:09:30.567861Z"
    }
   },
   "outputs": [],
   "source": [
    "class QiskitCircuit():\n",
    "    \n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        # --- Circuit definition ---\n",
    "        self.circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        self.n_qubits = n_qubits\n",
    "        self.theta0 = Parameter('Theta0')\n",
    "        self.theta1 = Parameter('Theta1')\n",
    "        self.theta2 = Parameter('Theta2')\n",
    "        self.theta3 = Parameter('Theta3')\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        self.circuit.h(all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        self.circuit.ry(self.theta0, 0)\n",
    "        self.circuit.ry(self.theta1, 1)\n",
    "        self.circuit.ry(self.theta2, 2)\n",
    "        self.circuit.ry(self.theta3, 3)\n",
    "        self.circuit.barrier()\n",
    "        \n",
    "#         # Apply controlled-unitary\n",
    "# #         uc=ry(self.theta4, 4).to_gate().control(4)\n",
    "# #         self.circuit.append(uc, [0,1,2,3,4])\n",
    "#         self.circuit.ry(self.theta4, 4).to_gate().control(4)\n",
    "    \n",
    "        self.circuit.barrier()\n",
    "        self.circuit.measure_all()\n",
    "        # ---------------------------\n",
    "        \n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "        \n",
    "    def N_qubit_expectation_Z(self,counts, shots, nr_qubits):\n",
    "        expects = np.zeros(nr_qubits)\n",
    "        for key in counts.keys():\n",
    "            perc = counts[key]/shots\n",
    "            check = np.array([(float(key[i])-1/2)*2*perc for i in range(nr_qubits)])\n",
    "            expects += check   \n",
    "        return expects  \n",
    "    \n",
    "    def run(self, i):\n",
    "        params = i\n",
    "#         print('params = {}'.format(len(params)))\n",
    "        backend = Aer.get_backend('qasm_simulator')\n",
    "        job_sim = execute(self.circuit,\n",
    "                          self.backend,\n",
    "                          shots=self.shots,\n",
    "                          parameter_binds = [{self.theta0 : float(params[0]),\n",
    "                                              self.theta1 : float(params[1]),\n",
    "                                              self.theta2 : float(params[2]),\n",
    "                                              self.theta3 : float(params[3])}])\n",
    "        \n",
    "        result_sim = job_sim.result()\n",
    "        counts = result_sim.get_counts(self.circuit)\n",
    "        return self.N_qubit_expectation_Z(counts,self.shots,NUM_QUBITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation [pi/4]: [ 0.7208 -0.7006  0.7118 -0.6988]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌───┐ ░ ┌────────────┐ ░  ░  ░ ┌─┐         \n",
       "   q_0: ┤ H ├─░─┤ RY(Theta0) ├─░──░──░─┤M├─────────\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░ └╥┘┌─┐      \n",
       "   q_1: ┤ H ├─░─┤ RY(Theta1) ├─░──░──░──╫─┤M├──────\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░  ║ └╥┘┌─┐   \n",
       "   q_2: ┤ H ├─░─┤ RY(Theta2) ├─░──░──░──╫──╫─┤M├───\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░  ║  ║ └╥┘┌─┐\n",
       "   q_3: ┤ H ├─░─┤ RY(Theta3) ├─░──░──░──╫──╫──╫─┤M├\n",
       "        └───┘ ░ └────────────┘ ░  ░  ░  ║  ║  ║ └╥┘\n",
       "meas_0: ════════════════════════════════╩══╬══╬══╬═\n",
       "                                           ║  ║  ║ \n",
       "meas_1: ═══════════════════════════════════╩══╬══╬═\n",
       "                                              ║  ║ \n",
       "meas_2: ══════════════════════════════════════╩══╬═\n",
       "                                                 ║ \n",
       "meas_3: ═════════════════════════════════════════╩═\n",
       "                                                   </pre>"
      ],
      "text/plain": [
       "        ┌───┐ ░ ┌────────────┐ ░  ░  ░ ┌─┐         \n",
       "   q_0: ┤ H ├─░─┤ RY(Theta0) ├─░──░──░─┤M├─────────\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░ └╥┘┌─┐      \n",
       "   q_1: ┤ H ├─░─┤ RY(Theta1) ├─░──░──░──╫─┤M├──────\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░  ║ └╥┘┌─┐   \n",
       "   q_2: ┤ H ├─░─┤ RY(Theta2) ├─░──░──░──╫──╫─┤M├───\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░  ║  ║ └╥┘┌─┐\n",
       "   q_3: ┤ H ├─░─┤ RY(Theta3) ├─░──░──░──╫──╫──╫─┤M├\n",
       "        └───┘ ░ └────────────┘ ░  ░  ░  ║  ║  ║ └╥┘\n",
       "meas_0: ════════════════════════════════╩══╬══╬══╬═\n",
       "                                           ║  ║  ║ \n",
       "meas_1: ═══════════════════════════════════╩══╬══╬═\n",
       "                                              ║  ║ \n",
       "meas_2: ══════════════════════════════════════╩══╬═\n",
       "                                                 ║ \n",
       "meas_3: ═════════════════════════════════════════╩═\n",
       "                                                   "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = QiskitCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)\n",
    "print('Expected value for rotation [pi/4]: {}'.format(circuit.run([-np.pi/4, np.pi/4, -np.pi/4, np.pi/4])))\n",
    "circuit.circuit.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TorchCircuit()\n",
    "\n",
    "A pytorch layer always has two functions. One for the forward pass and one for the backward pass. The forward pass simply takes the Quantum Circuits variational parameters from the previous pytorch layer and runs the circuit on the defined hardware (defined in `QiskitCircuit.run()`) and returns the measurements from the quantum hardware.\n",
    "These measurements will be the inputs of the next pytorch layer.\n",
    "\n",
    "The backward pass returns the gradients of the quantum circuit. In this case here it is finite difference.\n",
    "\n",
    "the `forward_tensor` is saved from the forward pass. So we just have to do one evaluation of the Q-Circuit in the backpass for the finite difference.\n",
    "\n",
    "The `gradient` variable here is as well hard coded to 3 parameters. This should be updated in the future and made more general.\n",
    "\n",
    "The loop `for k in range(len(input_numbers)):` goes through all the parameters (in this case 3), and shifts them by a small $\\epsilon$. Then it runs the circuit and takes the diefferences of the ouput for the parameters $\\Theta$ and $\\Theta + \\epsilon$. This is the finite difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCircuit(Function):    \n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'QiskitCirc'):\n",
    "            ctx.QiskitCirc = QiskitCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n",
    "            \n",
    "        exp_value = ctx.QiskitCirc.run(i)\n",
    "        \n",
    "        result = torch.tensor([exp_value])\n",
    "        \n",
    "        ctx.save_for_backward(result, i)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "#         eps = 0.01\n",
    "        \n",
    "        forward_tensor, i = ctx.saved_tensors\n",
    "#         print('forward_tensor = {}'.format(forward_tensor))\n",
    "        input_numbers = i\n",
    "#         print('input_numbers = {}'.format(input_numbers))\n",
    "        gradients = torch.Tensor()\n",
    "        \n",
    "        for k in range(len(input_numbers)):\n",
    "            shift_right = input_numbers\n",
    "            shift_right[k] = input_numbers[k] + SHIFT\n",
    "            shift_left = input_numbers\n",
    "            shift_left[k] = input_numbers[k] - SHIFT\n",
    "            \n",
    "            expectation_right = ctx.QiskitCirc.run(shift_right)\n",
    "            expectation_left  = ctx.QiskitCirc.run(shift_left)\n",
    "#             print('expectation_right = {}, \\nexpectation_left = {}'.format(expectation_right,\n",
    "#                                                                           expectation_left))\n",
    "            \n",
    "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
    "            gradients = torch.cat((gradients, gradient.float()))\n",
    "# #             print(k)\n",
    "#             input_eps = input_numbers\n",
    "#             input_eps[k] = input_numbers[k] + eps\n",
    "# #             print('input_eps = {}'.format(input_eps))\n",
    "#             exp_value = ctx.QiskitCirc.run(input_eps)\n",
    "#             print('exp_value = {}'.format(exp_value))\n",
    "#             print('forward_tensor[0][k] = {}'.format(forward_tensor[0][k]))\n",
    "#             gradient = (exp_value - forward_tensor[0][k].item())\n",
    "#             gradients.append(gradient)\n",
    "            \n",
    "#         print('gradients = {}'.format(gradients))\n",
    "        result = torch.Tensor(gradients)\n",
    "\n",
    "        return result.float() * grad_output.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 after quantum layer: tensor([[0.7034, 0.7070, 0.7140, 0.7036]], dtype=torch.float64,\n",
      "       grad_fn=<TorchCircuitBackward>)\n",
      "x.grad = tensor([-0.0010, -0.0207, -0.0152,  0.0081])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.pi/4, np.pi/4, np.pi/4, np.pi/4], requires_grad=True)\n",
    "# x = torch.tensor([[0.0, 0.0, 0.0]], requires_grad=True)\n",
    "\n",
    "qc = TorchCircuit.apply\n",
    "y1 = qc(x)\n",
    "print('y1 after quantum layer: {}'.format(y1))\n",
    "y1 = nn.Linear(4,1)(y1.float())\n",
    "y1.backward()\n",
    "print('x.grad = {}'.format(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Quantum Circuit's Gradient Descent\n",
    "\n",
    "First, we want the \"neural net\" consisting of just the quantum circuit (with its 4 inputs and 4 outputs) and a linear layer (from 4 inputs to 1 output) to converge to a target value (-1). So, we define a cost function where the cost is defined as the square distance from the target value.\n",
    "\n",
    "`x` is the initialization of the parameters. Here again, this was hard coded such that every angle starts at $\\pi / 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:23<00:00,  4.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23ebef3eda0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = TorchCircuit.apply\n",
    "\n",
    "def cost(x):\n",
    "    target = -1\n",
    "    expval = qc(x)[0]\n",
    "    # simple linear layer: add up 4 outputs of quantum layer\n",
    "    val = expval[0] + expval[1] + expval[2] + expval[3]\n",
    "    return torch.abs(val - target) ** 2, expval\n",
    "\n",
    "x = torch.tensor([np.pi/4, np.pi/4, np.pi/4, np.pi/4], requires_grad=True)\n",
    "opt = torch.optim.Adam([x], lr=0.1)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "loss_list = []\n",
    "expval_list = []\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "# for i in range(num_epoch):\n",
    "    opt.zero_grad()\n",
    "    loss, expval = cost(x)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    expval_list.append(expval)\n",
    "#     print(loss.item())\n",
    "\n",
    "plt.plot(loss_list)\n",
    "# print('final parameters: {}'.format(expval_list))\n",
    "    \n",
    "# print(circuit(phi, theta))\n",
    "# print(cost(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST (0-1) Dataset\n",
    "\n",
    "**Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 500\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Concentrating on the first 100 samples\n",
    "n_samples = 250\n",
    "\n",
    "X_train = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                         transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# Leaving only labels 0 and 1 \n",
    "idx = np.append(np.where(X_train.targets == 0)[0][:n_samples], \n",
    "                np.where(X_train.targets == 1)[0][:n_samples])\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = X_train.targets[idx]\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "\n",
    "X_test = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                        transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "idx = np.append(np.where(X_test.targets == 0)[0][:n_samples], \n",
    "                np.where(X_test.targets == 1)[0][:n_samples])\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = X_test.targets[idx]\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network with Q-node\n",
    "\n",
    "This NN is  2 layers of ConvNN and a fully connected layer, with a Q-Node as a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, NUM_QUBITS)\n",
    "        self.qc = TorchCircuit.apply\n",
    "        self.fc3 = nn.Linear(NUM_QUBITS, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "#         x = qc(x[0])\n",
    "        x = self.fc3(x.float())\n",
    "        return x\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.fc2(x)\n",
    "#         x = torch.Tensor([[np.pi*torch.tanh(val) for val in x[0]]])\n",
    "# #         x = self.qft(x[0]) # quantum layer\n",
    "#         x = self.fc3(x.float())\n",
    "#         x = torch.softmax(x)\n",
    "    \n",
    "# #         print(x)\n",
    "#         return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # apply softmax\n",
    "        pred = F.softmax(self.forward(x))\n",
    "        ans = torch.argmax(pred).item()\n",
    "        print('pred = {}, highest_guess = {}'.format(pred, ans))\n",
    "        return torch.tensor(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "# optimizer = optim.Adam(network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "standard pytorch training loop.\n",
    "- Load data from train_loader. Which is this case a single example each step.\n",
    "- Forward pass through NN\n",
    "- Caluculate loss\n",
    "- Backprop and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [2%]\tLoss: 0.3065\n",
      "Training [4%]\tLoss: 0.0930\n",
      "Training [6%]\tLoss: 0.0512\n",
      "Training [8%]\tLoss: 0.0312\n",
      "Training [10%]\tLoss: 0.0145\n",
      "Training [12%]\tLoss: 0.0375\n",
      "Training [14%]\tLoss: 0.0177\n",
      "Training [16%]\tLoss: 0.0307\n",
      "Training [18%]\tLoss: 0.0095\n",
      "Training [20%]\tLoss: 0.0028\n",
      "Training [22%]\tLoss: 0.0044\n",
      "Training [24%]\tLoss: 0.0016\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "loss_list = []\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         print(batch_idx)\n",
    "        optimizer.zero_grad()        \n",
    "        # Forward pass\n",
    "        output = network(data)\n",
    "        # Calculating loss\n",
    "        loss = loss_func(output, target)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "    print('Training [{:.0f}%]\\tLoss: {:.4f}'.format(\n",
    "        100. * (epoch + 1) / epochs, loss_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Neg Log Likelihood Loss')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.title('Hybrid NN Training Convergence')\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Neg Log Likelihood Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy of NN\n",
    "\n",
    "The outcome is not always the same because the prediction is probabilistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5412, 0.4588]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5206, 0.4794]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4779, 0.5221]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4890, 0.5110]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4464, 0.5536]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4732, 0.5268]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4366, 0.5634]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4684, 0.5316]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5209, 0.4791]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5104, 0.4896]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4236, 0.5764]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4619, 0.5381]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5341, 0.4659]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5170, 0.4830]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5102, 0.4898]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5051, 0.4949]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4690, 0.5310]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4845, 0.5155]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5401, 0.4599]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5201, 0.4799]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4487, 0.5513]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4744, 0.5256]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4642, 0.5358]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4821, 0.5179]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4572, 0.5428]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4786, 0.5214]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4300, 0.5700]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4651, 0.5349]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5550, 0.4450]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5275, 0.4725]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4579, 0.5421]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4790, 0.5210]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5335, 0.4665]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5167, 0.4833]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5377, 0.4623]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5188, 0.4812]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5068, 0.4932]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5034, 0.4966]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4571, 0.5429]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4785, 0.5215]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4983, 0.5017]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4991, 0.5009]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4600, 0.5400]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4800, 0.5200]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4809, 0.5191]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4905, 0.5095]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4251, 0.5749]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4626, 0.5374]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5245, 0.4755]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5122, 0.4878]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4736, 0.5264]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4868, 0.5132]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4340, 0.5660]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4671, 0.5329]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4364, 0.5636]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4682, 0.5318]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4942, 0.5058]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4971, 0.5029]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4226, 0.5774]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4614, 0.5386]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4267, 0.5733]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4634, 0.5366]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3594, 0.6406]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4302, 0.5698]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5454, 0.4546]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5227, 0.4773]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4256, 0.5744]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4629, 0.5371]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5465, 0.4535]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5232, 0.4768]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.3531, 0.6469]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4271, 0.5729]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4326, 0.5674]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4663, 0.5337]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4357, 0.5643]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4679, 0.5321]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3694, 0.6306]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4351, 0.5649]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4063, 0.5937]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4533, 0.5467]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4257, 0.5743]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4629, 0.5371]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5031, 0.4969]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5015, 0.4985]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5237, 0.4763]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5119, 0.4881]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5208, 0.4792]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5104, 0.4896]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4973, 0.5027]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4987, 0.5013]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4412, 0.5588]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4706, 0.5294]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4761, 0.5239]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4880, 0.5120]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4746, 0.5254]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4873, 0.5127]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5286, 0.4714]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5143, 0.4857]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5103, 0.4897]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5051, 0.4949]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4473, 0.5527]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4737, 0.5263]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5936, 0.4064]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5467, 0.4533]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5875, 0.4125]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5437, 0.4563]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4278, 0.5722]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4640, 0.5360]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4714, 0.5286]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4857, 0.5143]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4975, 0.5025]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4988, 0.5012]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4610, 0.5390]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4805, 0.5195]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4777, 0.5223]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4888, 0.5112]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5107, 0.4893]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5053, 0.4947]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4414, 0.5586]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4707, 0.5293]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5462, 0.4538]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5231, 0.4769]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4813, 0.5187]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4907, 0.5093]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4341, 0.5659]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4671, 0.5329]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4366, 0.5634]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4683, 0.5317]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4943, 0.5057]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4971, 0.5029]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4127, 0.5873]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4564, 0.5436]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4656, 0.5344]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4828, 0.5172]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5704, 0.4296]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5351, 0.4649]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4899, 0.5101]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4950, 0.5050]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4287, 0.5713]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4644, 0.5356]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4345, 0.5655]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4673, 0.5327]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4802, 0.5198]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4901, 0.5099]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4893, 0.5107]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4946, 0.5054]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4859, 0.5141]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4929, 0.5071]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4948, 0.5052]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4974, 0.5026]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3975, 0.6025]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4489, 0.5511]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4559, 0.5441]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4780, 0.5220]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5261, 0.4739]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5131, 0.4869]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4292, 0.5708]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4647, 0.5353]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5190, 0.4810]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5095, 0.4905]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edward\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4262, 0.5738]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4631, 0.5369]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5078, 0.4922]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5039, 0.4961]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5467, 0.4533]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5233, 0.4767]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4590, 0.5410]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4795, 0.5205]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5108, 0.4892]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5054, 0.4946]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4821, 0.5179]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4911, 0.5089]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4643, 0.5357]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4822, 0.5178]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4430, 0.5570]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4715, 0.5285]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4235, 0.5765]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4618, 0.5382]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4265, 0.5735]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4633, 0.5367]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4486, 0.5514]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4743, 0.5257]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4472, 0.5528]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4736, 0.5264]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4938, 0.5062]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4969, 0.5031]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4365, 0.5635]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4683, 0.5317]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4986, 0.5014]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4993, 0.5007]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5310, 0.4690]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5155, 0.4845]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5179, 0.4821]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5089, 0.4911]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4233, 0.5767]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4617, 0.5383]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3994, 0.6006]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4499, 0.5501]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5394, 0.4606]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5197, 0.4803]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4806, 0.5194]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4903, 0.5097]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4431, 0.5569]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4716, 0.5284]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4655, 0.5345]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4827, 0.5173]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3919, 0.6081]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4461, 0.5538]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4762, 0.5238]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4881, 0.5119]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5112, 0.4888]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5056, 0.4944]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5052, 0.4948]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5026, 0.4974]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5192, 0.4808]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5096, 0.4904]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5055, 0.4945]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5027, 0.4973]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4200, 0.5800]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4601, 0.5399]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4842, 0.5158]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4921, 0.5079]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4880, 0.5120]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4940, 0.5060]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4605, 0.5395]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4803, 0.5197]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5186, 0.4814]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5093, 0.4907]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4654, 0.5346]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4827, 0.5173]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4530, 0.5470]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4765, 0.5235]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4644, 0.5356]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4822, 0.5178]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4267, 0.5733]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4634, 0.5366]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4864, 0.5136]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4932, 0.5068]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5242, 0.4758]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5121, 0.4879]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4557, 0.5443]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4778, 0.5222]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4483, 0.5517]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4742, 0.5258]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4535, 0.5465]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4768, 0.5232]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4994, 0.5006]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4997, 0.5003]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4872, 0.5128]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4936, 0.5064]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4396, 0.5604]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4699, 0.5301]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4094, 0.5906]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4548, 0.5452]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4635, 0.5365]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4817, 0.5183]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4152, 0.5848]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4577, 0.5423]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4301, 0.5699]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4651, 0.5349]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4366, 0.5634]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4683, 0.5317]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4651, 0.5349]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4826, 0.5174]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5039, 0.4961]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5019, 0.4981]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4875, 0.5125]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4938, 0.5062]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4418, 0.5582]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4709, 0.5291]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4994, 0.5006]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4997, 0.5003]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3977, 0.6023]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4490, 0.5510]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4196, 0.5804]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4599, 0.5401]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4833, 0.5167]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4916, 0.5084]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4118, 0.5882]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4560, 0.5440]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4084, 0.5916]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4543, 0.5457]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4747, 0.5253]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4873, 0.5127]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4909, 0.5091]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4955, 0.5045]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5415, 0.4585]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5208, 0.4792]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4886, 0.5114]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4943, 0.5057]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5027, 0.4973]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5013, 0.4987]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4270, 0.5730]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4636, 0.5364]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4747, 0.5253]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4873, 0.5127]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4757, 0.5243]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4878, 0.5122]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4861, 0.5139]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4930, 0.5070]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5340, 0.4660]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5170, 0.4830]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4595, 0.5405]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4797, 0.5203]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4650, 0.5350]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4825, 0.5175]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4789, 0.5211]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4894, 0.5106]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4846, 0.5154]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4923, 0.5077]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4533, 0.5467]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4767, 0.5233]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4703, 0.5297]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4851, 0.5149]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4779, 0.5221]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4890, 0.5110]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4459, 0.5541]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4730, 0.5270]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4524, 0.5476]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4762, 0.5238]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5108, 0.4892]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5054, 0.4946]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4129, 0.5871]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4566, 0.5434]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4940, 0.5060]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4970, 0.5030]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3942, 0.6058]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4473, 0.5527]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4618, 0.5382]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4809, 0.5191]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3954, 0.6046]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4479, 0.5521]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5649, 0.4351]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5324, 0.4676]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4927, 0.5073]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4963, 0.5037]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4189, 0.5811]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4595, 0.5405]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4573, 0.5427]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4786, 0.5214]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3936, 0.6064]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4470, 0.5530]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5100, 0.4900]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5050, 0.4950]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4847, 0.5153]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4923, 0.5077]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4147, 0.5853]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4574, 0.5426]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4751, 0.5249]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4875, 0.5125]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4527, 0.5473]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4764, 0.5236]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4885, 0.5115]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4943, 0.5057]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4797, 0.5203]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4898, 0.5102]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4973, 0.5027]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4987, 0.5013]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4824, 0.5176]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4912, 0.5088]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4912, 0.5088]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4956, 0.5044]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4764, 0.5236]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4882, 0.5118]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4354, 0.5646]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4677, 0.5323]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4765, 0.5235]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4882, 0.5118]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4960, 0.5040]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4980, 0.5020]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5064, 0.4936]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5032, 0.4968]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4770, 0.5230]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4885, 0.5115]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4198, 0.5802]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4600, 0.5400]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3754, 0.6246]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4380, 0.5620]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4856, 0.5144]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4928, 0.5072]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4257, 0.5743]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4629, 0.5371]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4887, 0.5113]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4943, 0.5057]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4700, 0.5300]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4850, 0.5150]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4617, 0.5383]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4808, 0.5192]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5144, 0.4856]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5072, 0.4928]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5107, 0.4893]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5053, 0.4947]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4892, 0.5108]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4946, 0.5054]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4371, 0.5629]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4686, 0.5314]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4253, 0.5747]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4627, 0.5373]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3862, 0.6138]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4433, 0.5567]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4237, 0.5763]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4619, 0.5381]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5403, 0.4597]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5201, 0.4799]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4319, 0.5681]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4660, 0.5340]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3768, 0.6232]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4387, 0.5613]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4988, 0.5012]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4994, 0.5006]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4721, 0.5279]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4860, 0.5140]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3932, 0.6068]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4468, 0.5532]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5138, 0.4862]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5069, 0.4931]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4493, 0.5507]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4746, 0.5254]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4950, 0.5050]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4975, 0.5025]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4406, 0.5594]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4703, 0.5297]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3749, 0.6251]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4378, 0.5622]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5293, 0.4707]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5147, 0.4853]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4829, 0.5171]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4915, 0.5085]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4946, 0.5054]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4973, 0.5027]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4585, 0.5415]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4793, 0.5207]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4366, 0.5634]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4684, 0.5316]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4647, 0.5353]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4823, 0.5177]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4519, 0.5481]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4760, 0.5240]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4910, 0.5090]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4955, 0.5045]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3954, 0.6046]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4479, 0.5521]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4514, 0.5486]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4757, 0.5243]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4156, 0.5844]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4579, 0.5421]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4937, 0.5063]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4968, 0.5032]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4634, 0.5366]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4817, 0.5183]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4144, 0.5856]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4573, 0.5427]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4795, 0.5205]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4897, 0.5103]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4644, 0.5356]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4822, 0.5178]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5343, 0.4657]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5172, 0.4828]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4620, 0.5380]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4810, 0.5190]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4886, 0.5114]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4943, 0.5057]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5598, 0.4402]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5299, 0.4701]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4941, 0.5059]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4971, 0.5029]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5162, 0.4838]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5081, 0.4919]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4501, 0.5499]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4751, 0.5249]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4234, 0.5766]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4618, 0.5382]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4353, 0.5647]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4677, 0.5323]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4633, 0.5367]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4817, 0.5183]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4524, 0.5476]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4762, 0.5238]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5729, 0.4271]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5364, 0.4636]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4636, 0.5364]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4818, 0.5182]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5037, 0.4963]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5018, 0.4982]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4996, 0.5004]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4998, 0.5002]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5996, 0.4004]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5497, 0.4503]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5423, 0.4577]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5211, 0.4789]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4826, 0.5174]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4913, 0.5087]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4370, 0.5630]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4685, 0.5315]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4948, 0.5052]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4974, 0.5026]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4325, 0.5675]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4663, 0.5337]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4859, 0.5141]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4929, 0.5071]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4621, 0.5379]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4811, 0.5189]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4858, 0.5142]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4929, 0.5071]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4505, 0.5495]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4753, 0.5247]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4630, 0.5370]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4815, 0.5185]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4431, 0.5569]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4716, 0.5284]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4015, 0.5985]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4509, 0.5491]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5553, 0.4447]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5276, 0.4724]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4647, 0.5353]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4823, 0.5177]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4640, 0.5360]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4820, 0.5180]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5157, 0.4843]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5078, 0.4922]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5517, 0.4483]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5258, 0.4742]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4724, 0.5276]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4862, 0.5138]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4492, 0.5508]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4746, 0.5254]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4598, 0.5402]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4799, 0.5201]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5275, 0.4725]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5137, 0.4863]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4923, 0.5077]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4961, 0.5039]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4492, 0.5508]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4746, 0.5254]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5623, 0.4377]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5311, 0.4689]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4014, 0.5986]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4508, 0.5492]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4845, 0.5155]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4922, 0.5078]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4577, 0.5423]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4789, 0.5211]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3373, 0.6627]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4194, 0.5806]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4794, 0.5206]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4897, 0.5103]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4041, 0.5959]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4522, 0.5478]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4988, 0.5012]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4994, 0.5006]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4346, 0.5654]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4673, 0.5327]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5071, 0.4929]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5036, 0.4964]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5746, 0.4254]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5372, 0.4628]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4610, 0.5390]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4805, 0.5195]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4722, 0.5278]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4861, 0.5139]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5438, 0.4562]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5219, 0.4781]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4602, 0.5398]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4801, 0.5199]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4667, 0.5333]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4834, 0.5166]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4680, 0.5320]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4840, 0.5160]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4327, 0.5673]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4664, 0.5336]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4729, 0.5271]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4865, 0.5135]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4703, 0.5297]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4851, 0.5149]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4717, 0.5283]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4859, 0.5141]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5224, 0.4776]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5112, 0.4888]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5183, 0.4817]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5092, 0.4908]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5532, 0.4468]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5266, 0.4734]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5095, 0.4905]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5047, 0.4953]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5008, 0.4992]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5004, 0.4996]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4642, 0.5358]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4821, 0.5179]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4585, 0.5415]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4793, 0.5207]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4847, 0.5153]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4923, 0.5077]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4852, 0.5148]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4926, 0.5074]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4117, 0.5883]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4559, 0.5441]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5079, 0.4921]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5039, 0.4961]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.3874, 0.6126]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4440, 0.5560]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4967, 0.5033]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4984, 0.5016]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5469, 0.4531]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5235, 0.4765]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4647, 0.5353]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4824, 0.5176]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4386, 0.5614]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4693, 0.5307]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4168, 0.5832]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4585, 0.5415]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5027, 0.4973]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5014, 0.4986]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4530, 0.5470]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4765, 0.5235]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4417, 0.5583]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4709, 0.5291]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4470, 0.5530]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4735, 0.5265]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4894, 0.5106]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4947, 0.5053]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4313, 0.5687]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4657, 0.5343]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5635, 0.4365]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5317, 0.4683]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4330, 0.5670]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4665, 0.5335]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3725, 0.6275]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4366, 0.5634]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5693, 0.4307]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5346, 0.4654]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4182, 0.5818]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4592, 0.5408]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4251, 0.5749]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4626, 0.5374]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4240, 0.5760]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4621, 0.5379]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4547, 0.5453]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4773, 0.5227]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5488, 0.4512]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5244, 0.4756]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4765, 0.5235]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4882, 0.5118]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4317, 0.5683]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4659, 0.5341]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5146, 0.4854]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5073, 0.4927]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5005, 0.4995]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5003, 0.4997]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5114, 0.4886]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5057, 0.4943]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4958, 0.5042]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4979, 0.5021]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4625, 0.5375]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4813, 0.5187]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4538, 0.5462]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4769, 0.5231]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5044, 0.4956]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5022, 0.4978]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4757, 0.5243]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4879, 0.5121]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4173, 0.5827]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4587, 0.5413]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4529, 0.5471]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4765, 0.5235]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3890, 0.6110]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4447, 0.5553]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4785, 0.5215]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4893, 0.5107]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5239, 0.4761]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5120, 0.4880]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4386, 0.5614]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4693, 0.5307]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5167, 0.4833]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5083, 0.4917]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4853, 0.5147]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4926, 0.5074]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5210, 0.4790]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5105, 0.4895]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4405, 0.5595]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4703, 0.5297]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4905, 0.5095]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4953, 0.5047]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5033, 0.4967]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5017, 0.4983]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5224, 0.4776]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5112, 0.4888]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4617, 0.5383]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4808, 0.5192]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4431, 0.5569]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4716, 0.5284]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4208, 0.5792]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4605, 0.5395]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4787, 0.5213]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4894, 0.5106]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4971, 0.5029]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4985, 0.5015]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4769, 0.5231]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4884, 0.5116]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4358, 0.5642]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4679, 0.5321]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4677, 0.5323]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4839, 0.5161]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4240, 0.5760]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4621, 0.5379]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4648, 0.5352]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4824, 0.5176]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4458, 0.5542]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4729, 0.5271]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4318, 0.5682]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4660, 0.5340]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4192, 0.5808]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4597, 0.5403]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4546, 0.5454]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4773, 0.5227]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4485, 0.5515]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4743, 0.5257]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5356, 0.4644]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5178, 0.4822]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4810, 0.5190]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4905, 0.5095]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5449, 0.4551]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5224, 0.4776]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4450, 0.5550]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4725, 0.5275]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4985, 0.5015]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4992, 0.5008]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4482, 0.5518]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4741, 0.5259]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4763, 0.5237]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4881, 0.5119]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5353, 0.4647]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5176, 0.4824]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4743, 0.5257]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4871, 0.5129]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4540, 0.5460]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4770, 0.5230]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4737, 0.5263]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4869, 0.5131]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5106, 0.4894]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5053, 0.4947]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4623, 0.5377]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4812, 0.5188]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5495, 0.4505]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5247, 0.4753]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5013, 0.4987]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5007, 0.4993]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4520, 0.5480]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4760, 0.5240]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4798, 0.5202]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4899, 0.5101]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4697, 0.5303]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4848, 0.5152]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4247, 0.5753]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4624, 0.5376]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4696, 0.5304]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4848, 0.5152]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5200, 0.4800]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5100, 0.4900]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.3839, 0.6161]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4422, 0.5578]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4998, 0.5002]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4999, 0.5001]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5661, 0.4339]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5330, 0.4670]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5676, 0.4324]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5337, 0.4663]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4528, 0.5472]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4764, 0.5236]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4811, 0.5189]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4906, 0.5094]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.3995, 0.6005]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4499, 0.5501]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4536, 0.5464]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4768, 0.5232]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4998, 0.5002]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4999, 0.5001]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4951, 0.5049]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4975, 0.5025]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4282, 0.5718]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4642, 0.5358]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4445, 0.5555]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4723, 0.5277]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.4306, 0.5694]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4653, 0.5347]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5151, 0.4849]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5075, 0.4925]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5143, 0.4857]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5072, 0.4928]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.5020, 0.4980]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5010, 0.4990]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4020, 0.5980]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4511, 0.5489]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5294, 0.4706]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5147, 0.4853]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4484, 0.5516]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4742, 0.5258]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "tensor([[0.5548, 0.4452]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.5274, 0.4726]], grad_fn=<SoftmaxBackward>), highest_guess = 0\n",
      "tensor(0)\n",
      "tensor([[0.4447, 0.5553]], grad_fn=<SoftmaxBackward>)\n",
      "pred = tensor([[0.4724, 0.5276]], grad_fn=<SoftmaxBackward>), highest_guess = 1\n",
      "tensor(1)\n",
      "Performance on test data is is: 0\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "number = 0\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    number +=1\n",
    "    output = network.predict(data)\n",
    "    print(output)\n",
    "    accuracy += (output == target[0].item())*1\n",
    "    \n",
    "print(\"Performance on test data is is: {}\".format(accuracy/number))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edward\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4234, 0.5743, 0.0013, 0.0010]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-980a209e68cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples_show = 6\n",
    "count = 0\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(10, 3))\n",
    "\n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if count == n_samples_show:\n",
    "            break\n",
    "        output = network.predict(data)\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "\n",
    "        axes[count].imshow(data[0].numpy().squeeze(), cmap='gray')\n",
    "\n",
    "        axes[count].set_xticks([])\n",
    "        axes[count].set_yticks([])\n",
    "        axes[count].set_title('Predicted {}'.format(pred.item()))\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
