{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- coding: utf-8 --\n",
    "# This code is part of Qiskit.\n",
    "#\n",
    "# (C) Copyright IBM 2019.\n",
    "#\n",
    "# This code is licensed under the Apache License, Version 2.0. You may\n",
    "# obtain a copy of this license in the LICENSE.txt file in the root directory\n",
    "# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "#\n",
    "# Any modifications or derivative works of this code must retain this\n",
    "# copyright notice, and modified files need to carry a notice indicating\n",
    "# that they have been altered from the originals.\n",
    "#\n",
    "# Code adapted from QizGloria team, Qiskit Camp Europe 2019, updated by \n",
    "# Team Ube Pancake, Qiskit Summer Jam 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumRegister,QuantumCircuit,ClassicalRegister,execute\n",
    "from qiskit.circuit import Parameter,ControlledGate\n",
    "from qiskit import Aer\n",
    "import qiskit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "NUM_QUBITS = 4\n",
    "NUM_SHOTS = 10000\n",
    "SHIFT = 0.01\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "SIMULATOR = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to translate Q-Circuit parameters from pytorch back to QISKIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numbers(tensor_list):\n",
    "    num_list = []\n",
    "    for tensor in tensor_list:\n",
    "        num_list += [tensor.item()]\n",
    "    return num_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Contruct QuantumCircuit QFT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T16:09:30.598730Z",
     "start_time": "2019-10-01T16:09:30.567861Z"
    }
   },
   "outputs": [],
   "source": [
    "class QiskitCircuit():\n",
    "    \n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        # --- Circuit definition ---\n",
    "        self.circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        self.n_qubits = n_qubits\n",
    "        self.theta0 = Parameter('Theta0')\n",
    "        self.theta1 = Parameter('Theta1')\n",
    "        self.theta2 = Parameter('Theta2')\n",
    "        self.theta3 = Parameter('Theta3')\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        self.circuit.h(all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        self.circuit.ry(self.theta0, 0)\n",
    "        self.circuit.ry(self.theta1, 1)\n",
    "        self.circuit.ry(self.theta2, 2)\n",
    "        self.circuit.ry(self.theta3, 3)\n",
    "        self.circuit.barrier()\n",
    "        \n",
    "#         # Apply controlled-unitary\n",
    "# #         uc=ry(self.theta4, 4).to_gate().control(4)\n",
    "# #         self.circuit.append(uc, [0,1,2,3,4])\n",
    "#         self.circuit.ry(self.theta4, 4).to_gate().control(4)\n",
    "    \n",
    "        self.circuit.barrier()\n",
    "        self.circuit.measure_all()\n",
    "        # ---------------------------\n",
    "        \n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "        \n",
    "    def N_qubit_expectation_Z(self,counts, shots, nr_qubits):\n",
    "        expects = np.zeros(nr_qubits)\n",
    "        for key in counts.keys():\n",
    "            perc = counts[key]/shots\n",
    "            check = np.array([(float(key[i])-1/2)*2*perc for i in range(nr_qubits)])\n",
    "            expects += check   \n",
    "        return expects  \n",
    "    \n",
    "    def run(self, i):\n",
    "        params = i\n",
    "#         print('params = {}'.format(len(params)))\n",
    "        backend = Aer.get_backend('qasm_simulator')\n",
    "        job_sim = execute(self.circuit,\n",
    "                          self.backend,\n",
    "                          shots=self.shots,\n",
    "                          parameter_binds = [{self.theta0 : float(params[0]),\n",
    "                                              self.theta1 : float(params[1]),\n",
    "                                              self.theta2 : float(params[2]),\n",
    "                                              self.theta3 : float(params[3])}])\n",
    "        \n",
    "        result_sim = job_sim.result()\n",
    "        counts = result_sim.get_counts(self.circuit)\n",
    "        return self.N_qubit_expectation_Z(counts,self.shots,NUM_QUBITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation [pi/4]: [ 0.7092 -0.7096  0.7032 -0.705 ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌───┐ ░ ┌────────────┐ ░  ░  ░ ┌─┐         \n",
       "   q_0: ┤ H ├─░─┤ RY(Theta0) ├─░──░──░─┤M├─────────\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░ └╥┘┌─┐      \n",
       "   q_1: ┤ H ├─░─┤ RY(Theta1) ├─░──░──░──╫─┤M├──────\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░  ║ └╥┘┌─┐   \n",
       "   q_2: ┤ H ├─░─┤ RY(Theta2) ├─░──░──░──╫──╫─┤M├───\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░  ║  ║ └╥┘┌─┐\n",
       "   q_3: ┤ H ├─░─┤ RY(Theta3) ├─░──░──░──╫──╫──╫─┤M├\n",
       "        └───┘ ░ └────────────┘ ░  ░  ░  ║  ║  ║ └╥┘\n",
       "meas_0: ════════════════════════════════╩══╬══╬══╬═\n",
       "                                           ║  ║  ║ \n",
       "meas_1: ═══════════════════════════════════╩══╬══╬═\n",
       "                                              ║  ║ \n",
       "meas_2: ══════════════════════════════════════╩══╬═\n",
       "                                                 ║ \n",
       "meas_3: ═════════════════════════════════════════╩═\n",
       "                                                   </pre>"
      ],
      "text/plain": [
       "        ┌───┐ ░ ┌────────────┐ ░  ░  ░ ┌─┐         \n",
       "   q_0: ┤ H ├─░─┤ RY(Theta0) ├─░──░──░─┤M├─────────\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░ └╥┘┌─┐      \n",
       "   q_1: ┤ H ├─░─┤ RY(Theta1) ├─░──░──░──╫─┤M├──────\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░  ║ └╥┘┌─┐   \n",
       "   q_2: ┤ H ├─░─┤ RY(Theta2) ├─░──░──░──╫──╫─┤M├───\n",
       "        ├───┤ ░ ├────────────┤ ░  ░  ░  ║  ║ └╥┘┌─┐\n",
       "   q_3: ┤ H ├─░─┤ RY(Theta3) ├─░──░──░──╫──╫──╫─┤M├\n",
       "        └───┘ ░ └────────────┘ ░  ░  ░  ║  ║  ║ └╥┘\n",
       "meas_0: ════════════════════════════════╩══╬══╬══╬═\n",
       "                                           ║  ║  ║ \n",
       "meas_1: ═══════════════════════════════════╩══╬══╬═\n",
       "                                              ║  ║ \n",
       "meas_2: ══════════════════════════════════════╩══╬═\n",
       "                                                 ║ \n",
       "meas_3: ═════════════════════════════════════════╩═\n",
       "                                                   "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = QiskitCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)\n",
    "print('Expected value for rotation [pi/4]: {}'.format(circuit.run([-np.pi/4, np.pi/4, -np.pi/4, np.pi/4])))\n",
    "circuit.circuit.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TorchCircuit()\n",
    "\n",
    "A pytorch layer always has two functions. One for the forward pass and one for the backward pass. The forward pass simply takes the Quantum Circuits variational parameters from the previous pytorch layer and runs the circuit on the defined hardware (defined in `QiskitCircuit.run()`) and returns the measurements from the quantum hardware.\n",
    "These measurements will be the inputs of the next pytorch layer.\n",
    "\n",
    "The backward pass returns the gradients of the quantum circuit. In this case here it is finite difference.\n",
    "\n",
    "the `forward_tensor` is saved from the forward pass. So we just have to do one evaluation of the Q-Circuit in the backpass for the finite difference.\n",
    "\n",
    "The `gradient` variable here is as well hard coded to 3 parameters. This should be updated in the future and made more general.\n",
    "\n",
    "The loop `for k in range(len(input_numbers)):` goes through all the parameters (in this case 3), and shifts them by a small $\\epsilon$. Then it runs the circuit and takes the diefferences of the ouput for the parameters $\\Theta$ and $\\Theta + \\epsilon$. This is the finite difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCircuit(Function):    \n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'QiskitCirc'):\n",
    "            ctx.QiskitCirc = QiskitCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n",
    "            \n",
    "        exp_value = ctx.QiskitCirc.run(i)\n",
    "        \n",
    "        result = torch.tensor([exp_value])\n",
    "        \n",
    "        ctx.save_for_backward(result, i)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "#         eps = 0.01\n",
    "        \n",
    "        forward_tensor, i = ctx.saved_tensors\n",
    "#         print('forward_tensor = {}'.format(forward_tensor))\n",
    "        input_numbers = i\n",
    "#         print('input_numbers = {}'.format(input_numbers))\n",
    "        gradients = torch.Tensor()\n",
    "        \n",
    "        for k in range(len(input_numbers)):\n",
    "            shift_right = input_numbers\n",
    "            shift_right[k] = input_numbers[k] + SHIFT\n",
    "            shift_left = input_numbers\n",
    "            shift_left[k] = input_numbers[k] - SHIFT\n",
    "            \n",
    "            expectation_right = ctx.QiskitCirc.run(shift_right)\n",
    "            expectation_left  = ctx.QiskitCirc.run(shift_left)\n",
    "#             print('expectation_right = {}, \\nexpectation_left = {}'.format(expectation_right,\n",
    "#                                                                           expectation_left))\n",
    "            \n",
    "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
    "            gradients = torch.cat((gradients, gradient.float()))\n",
    "# #             print(k)\n",
    "#             input_eps = input_numbers\n",
    "#             input_eps[k] = input_numbers[k] + eps\n",
    "# #             print('input_eps = {}'.format(input_eps))\n",
    "#             exp_value = ctx.QiskitCirc.run(input_eps)\n",
    "#             print('exp_value = {}'.format(exp_value))\n",
    "#             print('forward_tensor[0][k] = {}'.format(forward_tensor[0][k]))\n",
    "#             gradient = (exp_value - forward_tensor[0][k].item())\n",
    "#             gradients.append(gradient)\n",
    "            \n",
    "#         print('gradients = {}'.format(gradients))\n",
    "        result = torch.Tensor(gradients)\n",
    "\n",
    "        return result.float() * grad_output.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 after quantum layer: tensor([[0.7016, 0.7112, 0.7028, 0.7104]], dtype=torch.float64,\n",
      "       grad_fn=<TorchCircuitBackward>)\n",
      "x.grad = tensor([-0.0025, -0.0060,  0.0027, -0.0024])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.pi/4, np.pi/4, np.pi/4, np.pi/4], requires_grad=True)\n",
    "# x = torch.tensor([[0.0, 0.0, 0.0]], requires_grad=True)\n",
    "\n",
    "qc = TorchCircuit.apply\n",
    "y1 = qc(x)\n",
    "print('y1 after quantum layer: {}'.format(y1))\n",
    "y1 = nn.Linear(4,1)(y1.float())\n",
    "y1.backward()\n",
    "print('x.grad = {}'.format(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Quantum Circuit's Gradient Descent\n",
    "\n",
    "First, we want the \"neural net\" consisting of just the quantum circuit (with its 4 inputs and 4 outputs) and a linear layer (from 4 inputs to 1 output) to converge to a target value (-1). So, we define a cost function where the cost is defined as the square distance from the target value.\n",
    "\n",
    "`x` is the initialization of the parameters. Here again, this was hard coded such that every angle starts at $\\pi / 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:25<00:00,  3.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x231528852e8>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = TorchCircuit.apply\n",
    "\n",
    "def cost(x):\n",
    "    target = -1\n",
    "    expval = qc(x)[0]\n",
    "    # simple linear layer: add up 4 outputs of quantum layer\n",
    "    val = expval[0] + expval[1] + expval[2] + expval[3]\n",
    "    return torch.abs(val - target) ** 2, expval\n",
    "\n",
    "x = torch.tensor([np.pi/4, np.pi/4, np.pi/4, np.pi/4], requires_grad=True)\n",
    "opt = torch.optim.Adam([x], lr=0.1)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "loss_list = []\n",
    "expval_list = []\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "# for i in range(num_epoch):\n",
    "    opt.zero_grad()\n",
    "    loss, expval = cost(x)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    expval_list.append(expval)\n",
    "#     print(loss.item())\n",
    "\n",
    "plt.plot(loss_list)\n",
    "# print('final parameters: {}'.format(expval_list))\n",
    "    \n",
    "# print(circuit(phi, theta))\n",
    "# print(cost(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST\n",
    "\n",
    "In this code we can not handle batches yet.\n",
    "This should be implemented as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 100\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size_train = 1\n",
    "batch_size_test = 1\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "n_datapoints = 100\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()])\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "labels = mnist_trainset.targets #get labels\n",
    "labels = labels.numpy()\n",
    "idx1 = np.where(labels == 0) #search all zeros\n",
    "idx2 = np.where(labels == 1) # search all ones\n",
    "idx = np.concatenate((idx1[0][0:n_datapoints//2],idx2[0][0:n_datapoints//2])) # concatenate their indices\n",
    "mnist_trainset.targets = labels[idx] \n",
    "mnist_trainset.data = mnist_trainset.data[idx]\n",
    "\n",
    "print(mnist_trainset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network with Q-node\n",
    "\n",
    "This NN is  2 layers of ConvNN and a fully connected layer, with a Q-Node as a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, NUM_QUBITS)\n",
    "        self.qft = TorchCircuit.apply\n",
    "        self.fc3 = nn.Linear(NUM_QUBITS, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "#         print(x)\n",
    "        x = self.qft(x[0]) # quantum layer\n",
    "#         print(x)\n",
    "        x = self.fc3(x.float())\n",
    "        print(x)\n",
    "        x = torch.sigmoid(x)\n",
    "#         print(x)\n",
    "#         x = nn.LogSoftmax()(x)\n",
    "#         x = torch.argmax(x)\n",
    "#         x = torch.Tensor([x])\n",
    "#         print(x)\n",
    "        x = torch.cat((x, 1-x), -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "# optimizer = optim.Adam(network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "standard pytorch training loop.\n",
    "- Load data from train_loader. Which is this case a single example each step.\n",
    "- Forward pass through NN\n",
    "- Caluculate loss\n",
    "- Backprop and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2188]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2024]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2266]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2107]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2719]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2509]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2307]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2365]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1988]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1938]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2118]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1981]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1489]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2302]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1836]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2005]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1543]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1639]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1716]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1705]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1772]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2056]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2349]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1881]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1477]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1958]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1757]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1699]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1602]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2219]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1853]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1645]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1911]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2234]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1854]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1886]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1877]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1617]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1722]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2057]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1727]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2024]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2136]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1945]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2058]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1888]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2078]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1801]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1927]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2190]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2551]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2426]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2706]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2168]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2298]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2193]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2283]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2340]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2364]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2738]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2368]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2249]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2273]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1946]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2767]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2262]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2373]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2344]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2158]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1905]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2228]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2268]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2524]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2657]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1866]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1959]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1957]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2049]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1667]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2727]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2368]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2389]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1987]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1993]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1544]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1942]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2061]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1815]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2328]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1970]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2298]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2342]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2020]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2261]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1978]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2167]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2328]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2260]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2406]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2251]], grad_fn=<AddmmBackward>)\n",
      "Training [5%]\tLoss: -0.4973\n",
      "tensor([[-0.2153]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1969]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1949]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2312]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2465]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2411]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2302]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2091]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2132]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2212]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2456]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1972]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2014]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2135]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1744]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2442]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1950]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2079]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2375]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2107]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1842]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2150]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2009]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2071]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2303]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2330]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2072]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2118]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2046]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1871]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2342]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2213]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2206]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2155]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1842]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2367]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2314]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2045]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2079]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2001]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2033]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2259]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2206]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2206]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2133]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2293]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2192]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2396]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2532]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2914]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2102]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2574]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2812]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2297]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2217]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2381]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2752]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2178]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2376]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2007]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2205]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2264]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1968]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2759]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2297]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2364]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2353]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2311]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2260]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2270]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2337]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2556]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2583]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2147]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2338]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2576]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2389]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2343]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2470]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2988]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2513]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2145]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2185]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2269]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1967]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2097]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2241]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2246]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2148]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2223]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2063]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2475]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2308]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2294]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2067]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2035]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2270]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2158]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2182]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2191]], grad_fn=<AddmmBackward>)\n",
      "Training [10%]\tLoss: -0.4988\n",
      "tensor([[-0.2497]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2370]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2112]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.3120]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2508]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2239]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2341]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1733]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2368]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2310]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1871]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2163]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2332]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1840]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1795]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1860]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1975]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1902]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1956]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2124]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2303]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2182]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2269]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1908]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1947]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1453]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2020]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1968]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1907]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1985]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2162]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1955]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2083]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1695]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1941]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1875]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2039]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2306]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1943]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1838]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2248]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2435]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2266]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1909]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1889]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2239]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1995]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2137]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1996]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2224]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2097]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1906]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1854]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2534]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2469]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2084]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2354]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2122]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2014]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2189]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1927]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2401]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2437]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2401]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2218]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2268]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2263]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2486]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2354]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2588]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2197]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2419]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2944]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2194]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2333]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2231]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2502]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2368]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2037]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2094]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2338]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2143]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2504]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2517]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2172]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2286]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2436]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2135]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2363]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2166]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2070]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         print(batch_idx)\n",
    "        optimizer.zero_grad()        \n",
    "        # Forward pass\n",
    "        output = network(data)\n",
    "        # Calculating loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "    print('Training [{:.0f}%]\\tLoss: {:.4f}'.format(\n",
    "        100. * (epoch + 1) / epochs, loss_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.title('Hybrid NN Training Convergence')\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Neg Log Likelihood Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy of NN\n",
    "\n",
    "The outcome is not always the same because the prediction is probabilistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "number = 0\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    number +=1\n",
    "    output = network(data)\n",
    "    output = (output>0.5).float()\n",
    "    accuracy += (output[0][1].item() == target[0].item())*1\n",
    "    \n",
    "print(\"Performance on test data is is: {}\".format(accuracy/number))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_show = 6\n",
    "count = 0\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(10, 3))\n",
    "\n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if count == n_samples_show:\n",
    "            break\n",
    "        output = network(data)\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "\n",
    "        axes[count].imshow(data[0].numpy().squeeze(), cmap='gray')\n",
    "\n",
    "        axes[count].set_xticks([])\n",
    "        axes[count].set_yticks([])\n",
    "        axes[count].set_title('Predicted {}'.format(pred.item()))\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
